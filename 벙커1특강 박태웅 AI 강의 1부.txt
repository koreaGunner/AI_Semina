[박수] 안녕하십니까 여기가 사실은 제 제
강의가 시작된 곳이고 만들어진 곳이라고 할 수 있는데요 김어준
총수가 저를 붙잡고 안나 주지 않았으면 강의가 안 나올 수도 있었습니다 그때가 하루에도 논문이
수십개씩 쏟아지는 캄브리아기 그때 마 주 연속 강의를 하는
람에 고산 때 이후로 그만큼 열심히 공부해 본 적이 있나 싶으실 정도로
정말 열심히 해 가지고 책까지 쓰게 됐는데요 제 강의를 유튜브나 이런
쪽에서 들으신 분들은 비슷한 얘기를 또 듣게 될 겁니다 그게 그 그럴
수밖에 없는게 제가 생각할 때 가장 쉽게 요체를 담아서 설명을 하려면
이렇게 하는게 제일 낫겠다 하고 궁리에 궁리를 해서 설명을 드리는
것이어서 제가 그걸 바꿔 보기 위해서 애를 안 쓴 건 아닌데 그 그때마다
그렇게 썩 결과가 좋지 않았어요 제가 드리는 말씀을 다시 드리는 것에 대해서 용서를 구하고요
복습한다는 마음으로 들어주시면 고맙겠습니다 채피의 인공지능의 시대라는
제목입니다 그 채피가 인류 역사상 가장 빠른 속도로
사용자를 모았습니다 인스타그램에서 만든 트위터 비슷한게
있는데 걔는 그 인스타그램에 새로운 기능이라고 볼 수가 있는게 있고
그다음에 올라갔던만큼이나 빠르게 들었기 때문에 걔를 이거 경쟁자로
놓고 보기 좀 어렵고요 채지 PT 두 달 만에 1억 명을 모았죠 그래프를
보시면 굉장히 큰 차이가 나는 걸 볼 수 있을 겁니다 그전까지는
인스타그램이 리 사용자를 모았던 서비스인데요 저 정도로 사용자를 빨리 모은 덕분에 직원이 14명을 때
1.4조 페이스북에 인수 당했습니다 근데 보면 채체 피티는 그냥 전봇대
같지 않습니까 인스타그램하는 왜 이렇게 전 인류가
인공지능 서비스에 이렇게 열광을 하게 됐을까 그게 이제 첫 번째 질문이겠지만
타임지가 채지 피티를 커버 스토리로 나누는데 거기에 그 표지 기사 제목이
the AI arms race is Changing everything입니다 여기서 주목할게 AI 붐을 맞았다 그나
아니면 뭐 르네상스가 다시 돌아왔다 그나 뭐 그런 여러 가지 말들을 쓸
수 있잖아요 근데 왜 하필 암레이스 군비 경쟁이라고
했을까 왜 타임지는 지금의 붐을 AI 암저 레이스라고 표현했을까 이게 두
번째 질문입니다 그리고 현대 인공지능을 알기 위해서 꼭 알아둬야 할 13 마이 있는데
그게 아 클라크의 제 3 법칙입니다 아 클라크가 세 가지 법칙에 관해
얘기했는데요 세 번째 법칙이 충분히 발달한 과학 기술은 마법과 분할 수
없다 그 앞에 법칙들 재밌는데요 그 된다고 하는게 틀릴 가능성은 있지만
어 누군가가 뭐가 안 된다고 말할 때는 그건 대부분의 경우에 틀린다
되기가 쉽다는 거죠 그게이 법칙인가 그렇습니다 근데 아터 클라크가 쓰
SF 다 재밌습니다 이제제 한번 안 읽어 보셨으면 보십시오 엔비디아는 전
세계 GPU 시장의 90% 정도를 먹고 있는 독점 사실상 독점 기업입니다 GPU 그래픽 프로세싱
유닛이라는 건데요 그래픽을 처리하는데 특화된 칩입니다 그 GPU대
대비되는게 CPU 아고 있죠 CPU 이제 컴퓨터 운영 체제를 돌리는 집이잖아 컴퓨터 뭐 프린트하고
연결하고 계산도 하고 여러가지 일들을 하는 그럴 프로싱 유닛이죠 인텔 만든 집에
마이크로소프트 만든 운영체제 윈도우가 올라가는 윈텔 체제가 한참 동안 뭐
지금도 그렇지만 시장을 먹었던 적이 있죠 그게 이제 CPU GPU CPU 다른
것은 병렬 연산 그니까 병열로 대구모 계산을 굉장히
잘합니다 왜 그래픽 프로싱 유닛이 대규모 계산을 그렇게 잘
면 그 이미지를 확대하고 확대하고 확대하면 픽셀이 나오잖아요 그걸 화소라고
하는데 가령 천만 와서 와 있다고 치고 얘를 동영상으로 처리하면
동영상이란게 정지 화면을 연속적으로 굉장히 빠른 속도로 보여
주는게 동영상이 아아 그래서 굉장히 훌륭한 동영상이면 보통 1초 60번
보여 주거든요 그러니까 천만 화서는데 1초 60번 보여 주려면
6억 번 계산을 해야 되죠 1초 안에 6억 개의 화소에 대해서 계산을
할 수 있어야 동영상을 매끄럽게 보여 줄 수가 있거든요 그래서
GPU 병렬 계산을 굉장히 잘하는 특화된 그런 물건인데 원래는 게임용
pc 붙이는 거였어요 게임이 워낙 그래픽이 화려하니까 뭐 레이싱 게임 같은 하고
뭐 FPS 총쏘는거 하려면 화면처리 엄청나게 해줘야 되거든요 근데
그러다가 인공지능 시대를 맞아서이 친구가 대박이 나 버린 거예요 인공지능이 어마무지한 계산을 해야
되는데 그러다 보니까 GPU 밑도 것도 없이 필요하게 된 거예요 그래 지금
그 제가 인공지능 공부를 하지 않고 옛날에 일찍 감지 비디아 주식을
샀으면 지금 제가 여러분들 여기서 안 봐도 되는데 저를 보고 싶으면 발리로
왔어야 되는데 말이에요 이런게 많습니다 제가 예전에 그 소프트웨어 세상을
집어삼킨다 굉장히 예전에 나온 10몇 년 전에 나온 인터뷰인데 그때 테슬라
세상을 집어삼킬거다 오 정말 맞는 말이네 그래놓고 그걸 안 샀어요 그걸 샀으면 제가 지금 좀
리해 그 비디 GPU V 100이란게 있는데 얘가 1초에 125조 더하기
기합니다 실수란 건 이제 소수점이 있는 숫자를 말하죠 1초에 조은
더하기 빼기를 하는데 그다음에 나온게 A 100입니다 A 100은 딥러닝 그러니까 인공지능 쪽 연산에 있어서는
V 100보다 연산 능력이 배입니다 그런데 채치 피티가이 A 100을
만대를 썼습니다 그리고 100일 동안 학습을 했거든요 그러니까 125 조번
1초에 1초에 125 조번 곱하기 20배 곱하기 60초 곱하기 60분
곱기 24시간 곱기 100 하면 채집 피티가 사용한 총 연산량이
나옵니다 이게 무슨 말인지 아시겠죠 1초에 125조 * 20 * 60초
* 60분 * 24시간 * 101을 하면 g GPT 사용한 학습 연산량이
나오는 겁니다 미친 거죠 그 인공지능이 쓰는 알고리즘들 중에 대표적인 것 중에
하나가 몬테카 알고리즘입니다 얘가 어떤 물건인가 하면 우리는 한
변의 길이가 2미인관 원의 넓이를 구하라 그러면 쉽게 구할 수 있습니다 한 변의
길이가
2미인관 곱 반지름 곱하기 잖아요 그러니까 1 * 1 * 3.14 1호
그죠 근데 인공지능은 그렇게 구하지 않습니다 인공지능은 그 10만 개가
됐던 100만 개가 됐던 점을 아주 랜덤하게 그러니까 대단히 무작위하게
발생을 시켜서 저 정사 쏩니다 그러면 굉장히 고르게 10만 개의 점이
바뀌겠죠 아주 무작위하게 랜덤하게 발생을 시켰으니까이 박스 안에 10만
개가 굉장히 고르게 박힐 거란 말이에요 그러면 그 10만 개의 점
중에서 어떤 것은 저 원안에 박히고 어떤 것은 원밖에 바뀌겠죠 그러면 그 원안에 박힌 점의
숫 를서 그걸 10만 개로 나누는 겁니다 그리고 우리가 정사각형 넓이는
알잖아요 그 비율로 4제곱 미를 곱하면 원의 넓이가
나오겠죠 아니 그런데 우리가 반지름의 제곱 곱하기 원주율이 원의 넓이를
구하는 공식을 아는데 왜 그렇게 번잡스럽게 구하지라고 할 수 있는데
그렇게 번잡스럽게 구한게 반지름의 제곱 곱기 원줄보다 빨라요 왜냐면
얘는 1초에 31 조번 더하기 빼기 하는 애니까 세는게 빠른 애예요 그
인공지능이 하는 일이 굉장히 우아하고 고상하고 뭔가 물리학적이고 수학적이고
뭐 그럴 것 같지만 실제로 한 거을 까보면 대부분의 경우가 이렇게 굉장히
단순한 막노동을 어마무지한 속도로 하는 겁니다 그러니까 지금 인공지능이
거의 대부분 경사 화법이란 걸 쓰는데요 경사 법이 뭐냐하면 이렇게 쭉 내려와서 어디에 잖아요 다음 면
전후좌우를 다 살펴서 자기보다 낮은 지역이 있는가를 살펴서 낮은 지역이 있으면 그로 이동을 합니다 그 거기서
다시 전후좌우를 살펴서 또 낮은데가 있으면 또 이동을 해요 그렇게 해서
가장 낮은데 갈 때까지 계산을 되풀이하는게 경사 법입니다 그 지금 인공지능이 대부분
경사하강법을 쓰거든요 그러니까 이것도 엄청 무식하지 엄청 무식한데 무식해도
말이 되는게 1초에 312 조번 곱하기 20 * 60 * 60 * 뭐 이렇게 하니까 말이 되는 겁니다
인공지능이란게 사람의 정신으로 하는 일을 기계가 대신하게 해 주자라는 겁니다 그러니까 인공지능이 그러니까
뭐 경운기가 됐던 경운기 뭐 포크레인 이런 트랙터 이런 거는 인간의 몸을
기계가 대신하게 해주자는 거잖아요 근데 인공지능은 인간의 정신을 기계가 대신하게 하자는 겁니다 비교적
초반에 고양이 사진을 가려내라이 많은 사진들 중에서 고양이를 가려내라 이거
사실은 이런 질문은 사람 같으면 대여섯 살만 되면 개하고 고향을 쉽게
가려냅니다 그죠 그 얘기는 고양이라는 패턴
개라는 패턴이 있다는 거예요 그죠 만약에 고양이가
아침에는 물병처럼 생겼다가 저녁에는 의자처럼 생기고 한밤중에는 마이크처럼
생기면 우리가 고양이를 알아챌 수가 없겠죠 고양이라는 패턴이 있어요 근데
아마 여섯 살짜리 에보고 고양의 특징에 대해서 다 서술해 보라 그러면 못 할 겁니다 그렇지만 얘는 고양
계를 아주 쉽게 구분 그래서 초창기의 인공지능 시스템이 전문가
시스템이라게 있었는데 그때는 고향의 특징을 다 쳐 넣었어요 뭐 수염이
어떻게 생겼고 귀가 어떻게 생겼고 뭐 쭉 다 쳐 놓고 그 사진을 식별을
내서 각각의 특징에 해당되는 걸 체크를 해서 합산점수가 이상이면
고양이다 이런 식으로 가련하게 했어요 그 초반에는 굉장히 이런 방식이 작동을
잘하는 거 같았습니다 그러니까 몇 점까지는 쉽게 올라갔어요 그래서이
점수를 더 높이기 위해서 특징을 더 많이 넣고 더 정교하게 만들고 그렇게
해서 고양을 가려내 했는데 일정 점수가 지나니까 그다음에
더 도로 점수가 떨어지기 시작하는 겁니다 왜 그런가 하면 예외가 너무
많은 거예요 그 사람은 고양이가 교통사고를 당해서
다리가 하나 없어도 고양이지 알잖아요 귀를 잘라도 고양인줄 알고 꼬리가
없어도 고양인줄 알고 심지어 죽어 있어도 고향이라는 걸 알잖아요 근데 이런 걸 다 선을 도리가 없는 겁니다
그래서 그런 방식으로는 안 된다이 이런 방식은 x 문제를 풀 수 없다 그래 가지고 인공지능에 겨울이
두 번이 있었습니다 안된다 해가지고 각 10년씩 갔어요 그러다가
2000년대에 들어와서 GPU 개발이 됩니다 그러니까 엄청나게 많은 계산을
할 수 있는 기반이 생겼어요 물론 GPU 인공지능을 위해서 만들어진 건 아니었어요 그 당시에는 그냥 게이밍
PC 붙이는 용도였다 있는 제프 인턴이라는
교수님이 뇌를 흉내낸 딥러닝이라는 알고리즘을 만듭니다 모델을 만드는데이
두 개가 결합을 하면서 지금의 인공지능 품이 시작합니다
그 지금의 인공지능이 과거의 인공지능 다른 점이 모든 것을 기계한테 맡기는
거예요 그러니까 앞에는 고양의 특징을 다 넣다 그랬잖아요 지금의
인공지능은 가령 한 사진을 한 15만 장을 컴퓨터에 주고이 15만 장의
사진들 간의 차이점 다른 점을 네가 다 찾아내라라고 맡겨요 그 컴퓨터가 다른
점을 찾아낼 수 있을 거 아니요 그래서 가령 다른 점을 천만 개를 찾아냈다 은 1억개를 찾아냈다고 해요
특징들 천만개 혹은 1억개 중에 어떤 것은 고양이라는 성질에 기여를 하는 것이 있을 거고 어떤 건 고양이라는
성질이 전혀 귀하지 않는게 있을 거예요 예를 들어서 뭐 고양이가 이게 목에 예쁜 목줄을 달고 있으면 그건
고양이 다움에 기하지 않는 거잖아요 고양이 그림자도 고양이 아무 관계 없죠 그래서 천만 개가 됐던 1개가
됐던을 잡아낸 다음에들의 중 먹기는
거예요이 가중치를 먹기는 것을 매개 변수라고 그럽니다 가중치를 먹긴
다음에 여기에 각각의 특징들 차이점들이 가중치를 어떻게 할당을 할
때 얘가 고양이를 잘 찾아내는 가장 잘 찾아내는 가를 시뮬레이션을 하는
겁니다 그러니까 15만 장에다 매개변수가 천만 개면 15만장 곱기
만이죠 그니까 뭔가 잠깐 들어봐도 거 같잖아요 근데 얘는 1초에 312
조번 곱하기 20 * 60 * 60 * 24 뭐 이런
애니까 그것도 전기만 꽂아 놓으면지가 알아서 하니까 15만 장 곱하기
1천만이 해 봐야 별게 아닌 거예요 그러면 그 가중치의 조합 중에서
최적의 조합이 나올 거 아니에요 그죠 그 지금은 그 개나 고양이를
사람보다도 훨씬 더 잘 가려내는 수준까지 올라왔습니다 이게 이제 현대 인공지능이 하는
일이에요 근데 문제가 있습니다 이게 맞추기는 기가 막히게
잘 맞추는데 왜 잘 맞추는지 설명할 방법이 없습니다 그러니까 이게 왜 잘
맞추는지 설명할 수 있으려면 그 매개변수를 하나씩 다 열어서
왜이 여기는 0.00000 25점을 주고 그 옆에 거는 0.00 13점을
주고 그옆에 거는 0 00 1점을 줬는지 설명을 할 수 있어야 되잖아요
천만 개인데 천만 개는 되게 작은 거예요 보통 몇백억 개 뭐 몇 천억 개거 채
GPT 150억 개 그 GPT 4는 1조 8억개 그거를 언제 열어 보겠어요
열어본다 하더라도 앞에 걸 다 기억하고 있어야 되잖아 그래야지 점수를 하나씩 설명을 할 수 있을거
아네요 그래서 현대 인공지능의 가장 큰 특징이 설명할 수 없다요 그래서
설명할 수 있는 인공지능을 xai 불러요 explainable AI
그래서 AI AI 굉장히 중요한 장입니다 왜냐면 이게 왜 잘
작동하는지를 설명할 수 없다는 얘기는 언제 갑자기 작동을 잘하지 않게
될지도 알 수 없다 얘기거든요 그러니까 얘는 현재 인공지능은 미션
어떤 일에도 쓸 수 없어요 왜 잘 맞추는지 모르는 거고요 그 가 하는 일을 그 현대 인공지능이 하는 일을
한마디로 말해 보라 그러면 잠재된 패턴을 찾아내는 일을 한다라고 말할 수 있어요 그 현대 인공지능 하는
일을 수학적으로 표현하면 이렇습니다 저 끝에 보시면 주황색 화살표가
보이시죠 그 X Y 아는 두 개의 변수로 이루어진 그래프인데요 우리가
저렇게 그래프를 저 모양 연속된 모양을 그릴 수만 있으면 우리가
예측을 할 수 있습니다 가령 사과가 한 알에 1,000원이다
그러면 두 알이면 2,000 원이죠 5,000원이면 다섯 알이지 일곱 개면 7,000 원이죠 그런 지금
하나를 알면 나머지를 알 수 있죠 왜냐면 좀 화살표가 있으니까 그리고 변수가 하나 더
생기면 축을 하나 더 하면 됩니다 그죠 수학에서는 저렇게 축을 하나 보태는 것을 차원을 더한다고 합니다
그 인간은 3차원적인 존재니까 3차원을 넘어가면 상상할 수 없어요
4차원의 존재를 상상할 수 없잖아요 5차원 6차원 말할 것도 없고 그렇지만 수학에서는 무한대의 차원을
계산할 수 있습니다 축만 더하면 되니까요 저렇게 하나 더 거면 되죠
계속 축을 더 보태면 됩니다 그러니까 XY Z시면 조런 주황색에 연속된
다양체를 그려낼 수 있다면 우리는 예측할 수 있죠 XY 값을 알면 Z 찾아낼 수 있는 겁니다 뭔가 만날 거
아니요 어딘가에서 그죠 이해되시죠
근데 이게 이제 수천만 차원까지 가도 마찬가지예요 수천만 차원 가도 이렇게
연속된 다양체를 그려낼 수 있다면 우리는 예측을 할 수 있습니다 그러니까 고양이 사진을 가려내는
작업은 그 몇 천만 차원에 걸쳐서 저렇게 연속적인 다양체를 그리는
작업을 하는 겁니다 왜 저런 연속된 다양체를 그릴 수 있냐면요 고양이라는
잠재된 패턴이 있기 때문에요 우리는 고양이라는 잠재된 패턴이 있다는 걸 알고 있잖아요 그걸 수학으로 표현을
못하고 말로 표현을 못 할 뿐이지 그러니까 그걸 몇 천만 차원에 걸쳐서 더듬어서 찾아내는 거예요 예를 들어서
위에서 쫙 몇 천만 개가 내려오자아요 그러면서 경사 법을 써서 계속 그림을
그리다 보면 저런 그림을 그릴 수 있겠죠 몇천만 개 내려와서 경사
법으로 제각기 가장 낮은 데를 찾아가면 결국은 저렇게 이어진 연 된 다양체를 그릴 수가 있을 거
아니에요 그게 현대 인공지능이 하는 일입니다 그러니까 잠재된 패턴 있는
모든 곳에서 인공지능이 굉장히 잘 작동합니다 그 프로그래밍 같으면 저건
사람이 만든 규칙이 아아 C 됐든 자바가 됐든 자바스크립트가 됐든
파이썬이 됐든 인간이 만든 규칙이 거기는 분명한 패턴이 있겠죠 그럼 얘는
패턴을 찾는 애니까 코딩을 잘해요 제가 이제 it 업계에 일하고 있으니까 제 주변에 뛰어난 개발자가
굉장히 많을 거 아니네요이 친구들은 작년 말부터 다 이걸 쓰고 있습니다 그리고 자기 직원들한테도 돈을 주고
이걸 다 쓰게 시켜요이 친구들 하는 말이 최소한 3년 차 개발자 선호
명을 데리고 일하는 거 같다라고 해요 물론 이제이 친구들이 굉장히 뛰어난
개발자이기 때문에 얘가 뭔가 거짓말을 하거나 잘못된 일을 하면 금방 알아채 수 있어서 잘 쓰는 것도 있습니다
그러니까 설계를 할 줄 알고 디버깅을 할 수 있으면 예랑은 일을 굉장히 잘할 수 있는 거예요 근데 왜 한
명이 아니고 선인가 하면 코드가 아무리 쉬워도 길면
인간은 기본적으로 타이핑하는 시간이 있습니다 근데 얘는 그런게 없어요 그냥 토해 내거든요 몇십초 몇십 초만
주면 토해 내니까 어렵든 쉽든 똑같이 나와요 그러니까 명 이상의 효율이
나는 거죠 그리고 법률 무식도 마찬가지입니다이 직 인간이 만든 규칙이 굉장히 뚜렷한 패턴이 있잖아요
그러니까 쉽게 패턴을 찾아낼 수 있습니다 그 gpt4 같은게 미국
변호사 시험을 상위 10% 통과하는 이유도 워낙 분명한 패턴이 있기 때문에 가능한
일입니다 저널리즘 마찬가지입니다 기사를 쓰는게 꽤 창의적인 일 아니야 그렇지 않습니다 여러분들이 그 일기
예보를 날짜 가려놓고 보시면 절대로 며칠인지 못 맞출 거예요 농구시합
결과 야구시합 결과 제 주식 시장이 어땠다 전부 다 비슷 비슷비슷한 패턴을 갖고 있습니다 그러니까 이런
일들은 인공지능이 아주 잘하는 일이 됩니다 그 지금 말씀드린 이런
분야에서 그 일자리를 뺏기는 분이 굉장히 많이 나올 겁니다 내년 말까지
굉장히 큰 변화가 일어날 거예요 피할 수 없습니다 이게 이제 채치 피티에 대해서 얘기하기 전에 여러분들이
알아야 될 갖춰야 될 기본 지식이었습니다 이제부터 피티에 대해서 말씀을
드리겠습니다은 대화 입니다 그러니까 사람이 말을 하듯이 데이터를 집어넣을
수 있다라는 뜻인데 이게 굉장히 큰 의미를 갖고 있어요 제가 지금 이렇게 하는 말을 연어라고
합니다라고 해요 그니까 사람이 사람한테 하는 말을라고 해요 그럼 그 상대편에 뭐가
있냐 머신지가 있어요 기계어가 있습니다 기계한테 말을 걸기 위해서
우리가 익히는 말을 머신라고 합니다 뭐 스크트 C 파이썬 이런 것들 스트
이런 것들을 다 머신 지라고 하죠 체치 뷰티는 처음으로 사람의 말로
기계한테 말을 걸고 일을 시킬 수가 있는 그런 시스템이에요 제가 이렇게
말씀을 드리면 어 시리도 있잖아 뭐 말할 수 있는데 시리는 장난감이 아아
걔는 문자를 보내라 뭐 노래를 찾아주라 보다 더 조금만 한 단계 올라간 말을 하면 헛소리를 하기
시작합니다 미안한데 못 알아들었다 다시 해주라면 그 진심으로
대화형이다 근데 대화형 아닌 컴퓨 인공지능도 많이 있죠 아까 고양이 사진을 가려내라 이런 거는 분류형
이잖아요 그러니까 얘는 대화형지는 제너러티브
생성하는이란 뜻이 만들어 낸다 그림을 학습하면 그림을 그리고 글을 학습하면
글을 쓰고 동영상을 학습하면 동영상을 만들어냅니다 그 만들어내지 않는 인공지능도 있죠 알파폴드 같은
경우에는 단백질의 접힘을 예측하는 그런 인공 지능입니다 얘는 만들어내는
인공지능에는 프리트레인 사전 학습한이란 뜻입니다 뭘 사전 학습했는지 문서를
사전 학습했어요 그래서이 정도 사이즈의 언어를 학습한 애를 라지
랑지 모델 거대 언어 모델이라고 합니다 근데 모든 거대 언어 모델이
파운데이션 모델입니다 파운데이션이 뭐 기반 근거 뭐 이런 뜻이잖아요 왜
모든 거대 언어 모델이 파운데이션 모델인가 하면이 친구가 느닷없이 나타나는
능력을 보여주는 입니다 그 우리가 채치 비티에 열광한
두 번째 이유가 여기 있는데요 그러니까 학습 연산량이 10의 22 제곱을 넘어가는 순간 혹은 매개
변수가 천억 개를 넘으면 능력을 테스트하는 여러 가지 범주가 있는데요
그래프가 이렇게 가다가 툭니다 그것도 그냥 한 군데서만
튀는게 아니고 능력을 테스트는 여덟개 지표 이상에서 이렇게 가다가 툭
튀어요 그니까 그 수없 언어능력을 갑자기 보여주고 그전까지 보여주지
못했던 뛰어난 추론 능력을 보여주고 막 코딩을 하기 시작하고 막 그럽니다 근데 왜 그런지는 몰라요 그래서 걔를
느닷없이 나타난 능력라고 합니다 발성이라 이스를
성이라고도 하는데이 창성이 저는 굉장히 잘못된 번역이라고 생각해요
이게 마치 구제역과 같은 표현인데요 구제역이 어
면요 직역하면 발과 입병 그니까 그
가축에 발과 입에서 어떤 증상이 나타나는 병이에요 그게 구제역에 그걸
환자로 쓰면 입구자 발제자 써서 구제역이 근데 구역이라고 하면
사람들이 다 그게 뭐냐고 또 물어봐야 되잖아요 이놈을 영어처럼으면 그걸 누가 물어요
유치원생도 알지 창성이 딱 그런 번이요 그러면 그 뭔데 또 물어봐야
되잖아 느닷없이 나타난 능력 그러면 그걸 누가 먹겠냐고 튼 이상한 사람들이
많아요 하여튼 느닷없이 나타나는 능력이 있어서
얘가 그 사전 학습하지 않은 어떤 것에 대해서도 질문을 하면 되게 잘
대답을 합니다 마치 전부터 잘 알고 있던 것처럼 그래서 모든 분야에
걸쳐서 기반이 될 수 있는 모델이라고 해서 파운데이션 모델이라고 합니다 그니까 모든 라시 랑지 모델은
파운데이션 모델입니다 TN
트랜스포머라쳇 비는 장충 단거리를 쓸쓸히 혼자 걷고 있는데 저
건너편에서 김호준 총수가 오고 있었다라고 하면 핵심 키워드가 뭐예요
김어준 만났다 잖아요 그 앞에 그거 다 헛소리라고요 그 그다음에 올다
하가 뭔지는 김어준을 만났다는 것에 관해서 나올 건 뻔하잖아요 이게 키워드에 그러니까 어텐션 모델을
쓰면 연산을 굉장히 줄이면서도 정확성을 훨씬 높일 수가 있어요
그래서 대부분 에 트랜스포머 모델은 예외없이 어텐션 뭐 멀티헤드 어텐션
그러니까 여러 가지 헤드를 갖고 있어서 키워드를 저 멀리까지 잘 찾아낸다 그래서 멀티헤드 어텐션을 다
씁니다 그리고 지금 트랜스포머가 워낙 잘 작동을 해서 생성 인공지능은 그
예외 없이 다 트랜스포머를 쓴다고 보면 됩니다 근데 재밌는 거는 트랜스포머 모델도 어텐션 모델도 다
구글이 만든 거예요 구글이 만들어서 오픈 소스로깐 거를 오픈에 덜렁 집어와서 이런 걸 했습니다 그리고
모델은 공개 안 했어요 아주 채입니다 그 채치 피티는
매개변수가 150억 개입니다 그러니까 5조의 문서를 150억 개의
매개변수를 돌렸으니 그 a 100을 만대를 썼지만 학습하는 시간이 100일이
걸린 거예요 근데 그냥 한게 아니고 rhf 걸 썼습니다 RF from hum feed이라고
해서 인간의 드백을 통해서 강화학습을 라는 건데 어떤 방식으로 했냐면
사람이 작성한 굉장히 품질이 높은 질문 답변 세트를 10만 개를 만듭니다 그 돈이 굉장히 많이 들겠죠
어쨌든 굉장히 질이 좋은 질문 답변 세트를 10만 개를 만들어서 얘를 학습을 시킵니다 그러니까 그 조의
문서를 가지고 학습한 다음에 일어나는 일이에요 그 10만 개 문서 굉장히 높은
품질의 10만 개의 문서를 가지고 학습을 시킨 다음에 인간들이 질문을 하고 그럼 얘가 답을 할 거 아니에요
그 답에 대해서 1점부터 5점까지 점수를 먹여서 피드백을 하는 거예요 1점은 네가 절대로 하면 안 되는 답
5점은 참잘했어요 이런 거예요 그런 식으로 하면서 뭘
하냐면 훌륭한 말과 하면 안 되는 말 뭐 헤이트 스피라 뭐 비윤리적인
말이라든가 들어가 있는 말이든지 이런 것들을 걸러내는 것을 훈련을 시키는 거예요 이렇게 인공지능에게 인간의
윤리를 학습시키는 것을 정한다 그를 하는 과정에서 얘가
능력이 살짝 떨어져요 그니까 모범생들이 왜 학교 밖 나오면 세상을 잘 모르잖아요 그
얘를 약간 범으로 만드는 과정에서 불가피하게 능력이 좀 떨어집니다 저같이 되는 거예요 그렇게 해서
능력이 살짝 떨어지는 걸 먼라고 불러요 정렬 어쨌든 그렇게 정 시켜서
그피가 소리를 굉장히 적게 하고 되게 예의가 발라요 너희가 틀린 말 했잖아 그러면 즉시 사과합니다 얘가 되게
예의 바르게 말하고 뭔가 헤이트 스피치를 시키려고 유도하면 미안하다 뭐 이렇게 하고 잘 안 합니다 영 안
넘어온다는 뜻은 아니고 잘 안 넘어옵니다 근데 이것도 굉장히 돈이 많이 듭니다
일단 평가자가 여러 명일 거 아니에요 그 평가자들의 가치 기준이 일치해야
됩니다 그까 똑같은 답을 했는데 어떤 사람은 2점을 주고 어떤 사람은 5점을 주면
얘가 다중 인격자가 되거나 아니면 더러워서 일 못하겠다 너희들끼리 맞춰 와라 뭐 이러겠죠 뉴욕 타임스 보도에
따르면 이런저런 거 합쳐서 채치 패티를 학습시키는 3.7조 원이
들었다 그래요 베이 한데 몇천만 하거든요 걔가 만 된다가 그걸 돌리는
전기료가 얼마나 많이 들겠습니까 거기다 조개의 문서를 걸고 와서 집어 넣죠 마지막에
하느라고 석사급 이상에 아주 뛰어난 인재들을 대량으로 집어넣어서 10만
개의 높은 품질의 질문 답변 세트를 만들고 그걸 딱 알래 이까지 했으니까
돈이 어마무지하게 드는 작업입니다 그러니까 이쪽은 기본적으로 단위가 다
조아요 들어가는 돈도 3.7조 문서 학습한 것도 5조 개고
매개 변수는 뭐 150억대 그걸 학습 시키는데 어쩌고 저쩌고 이렇게 해서
기본 단위가니다 그리고 뭐이 개발한 사람들도 재산이다 좋요 그리고 몇 달
있다가 GPT 4가 나왔습니다 GPT 4는 그 미국 변호사 시험을 30%
통과해요 채치 비티는 GPT 3.5 버전인데 채치 피티도 미국 변호사
시험을 통과하는데 얘는 하 20% 통과 했거든요 근데이 현대 인공지능이
굉장히 웃기는 점이 많은데요 이쪽에 규모의 법칙이 작용을 합니다 규모의 법칙이 작용을 한다는게
어떤 뜻인가면 세가지 그니까 파워를 많이 집어넣으면 많이 집어 넣을수록
학습 데이터를 많이 집어넣으면 많이 집어 넣을수록 매개 변수를 양 숫자를
늘리면 늘릴수록 성능이 좋아져요 그래서 규모의 법칙이라고 합니다 모델을 별로 바꾸지 않아도 양을 그냥
때려 놓으면 더 잘 나와요 그 지금 사람들이 생각하기로는
GPT 3하고 GPT 4가 그 채 PT gbt 4가 모델이 그렇게 다르지 않을 거다라고 얘기를 합니다
그냥 얘가 이제 매개변수 1.8 쪽에
그리고을 아마 2 5천대 정도 썼을 거라는게 예측인자 않았으니까 근데 모델은
똑같이 하고 그냥 그런 투입한 자원만 그렇게
늘렸는데 그 츠 피티는 미국 변호사 시험을 하위 10% 통과하는데 얘는
상위 10% 통과는 거예요 그니까 이것도 굉장히 이상한 거예요 그래서 지금 그 지금
인공지능을 발명을 하는게 아니고 발견을 한다고 설명하기도 해요 그
발명을 한다 그러면 뭐가 어떻게 될지를 대강 짐작은 하고 이런저런 원리로 될 거야라고 해야 되는데
지금은 사실은 이게 되네 더 가깝거든요 마치 초전도체 같죠 이게 되네 더
가까워요 그 지금 뭐 세기 말인지 뭐가 이상한게 자꾸 나옵니다 설명을
못하는데 왜 되지 가 계속 나오고 있어요 여러분들이 채치 피티를 써
보셨으면 채치 피티한달 말을 들으신 적이 있을
겁니다 근데 GPT 4에 오면요 GPT 4에 한글이 채집 B 뒤에서
영어로 물었던 거보다 답이 좋게 나와요 그러니까 언어의 장벽이라는게 그렇게 높지 않더라라는 거죠 이전에는
그래도 한글하고 영어는 언어의 장벽이 있으니까라는 말이 통했죠 근데 이
결과를 보면 언어의 장벽이 그렇게 대단할까 하는 질문을 해볼 수 있는
거죠 근데 이제 보시면 그렇다 하더라도 그 gbt 4의 한글보다는
gbt 4의 영어가 훨씬 낫습니다 근데 체즈 BT 영어보다는 gbt 보에 한글이 났다는 거고 어쨌든
약간의 차이는 있습니다 그렇지만 채비에서 gbt 4까지
오는데 몇 달 안 걸렸거든요 그 지금 gbt 5 한다는 거 아니에요
뭐가 나올지 그리고 얘가 그 기존에는 입력창 그걸 프롬트 하죠 프롬트
3,000 단어까지 집어넣을 수 있었어요 질문으로 근데 얘는 25,000 단어
이상 집어넣을 수 있습니다 25,000 단면 책 반권 이거든요 그 얘가 이제인 컨텍스트 러닝이라는
하면 그 질문을 가지고 학습을 합니다 그걸 icl인 컨텍스트 러닝이라는
책 반권 정도를 질문으로 집어넣어서 물어볼 수 있는
거니까 내가 그간에 있었던 어떤 사정이라 그가 내가 갖고 있는 어떤 자료에 대해서 어지간한 거는 다
때려쳐 놓고 이걸 보고 뭐라고 뭐라고 질문을 할 수 있는 거죠
로드라는 얘 비슷한 채집 PT 보다는 성능이 뛰어난 어떤 인공지능이 있데 걔는 10만
단어까지 집어넣다 그래요 10만 단은 책이 한권이 넘죠 GPT 4에 그고
가장 다른 점 중에 하나 얘가 멀티 모델이라는 겁니다 멀티 모달이 뭐냐면 제츠 PT 텍스트만 입력을 받았잖아요
얘는 이미지를 입력을 받습니다 그래서 모드가 두 개다 해서 여러 개의 모드를 갖고 있다고 해서 멀티
모델입니다 gbt 4 이후로는 모든 생성 인공 지능들을 다 멀티 모달이 기본이 될 거예요 왜냐면 그냥
텍스트만 입력받아 가지고는 존재 가치가 없잖아요 채집
PT 테도 안 될 뿐더러 GPT 4까지 있으니까 그러니까 얘가 이미지를 인식한다
그래서 GPT 4를 발표하고 오픈 AI 쪽에서 GPT 4가 얼마나 일을 잘하냐 하면 하고 증거로 내 놓은게
이겁니다 이걸 이미지로 준 거예요이 이미지를 입력을 하고 야 여기서 뭐가
웃긴지 설명을 해 줘라고 물은 거예요 그근데 GPT 4가이 질문에 대답을
하려면 아주 여러 가지 일을 해야 됩니다 첫 번째로 이미지를 이해해야 됩니다 저 까만게 아이폰이 밑에
파란게 VJ 커넥터 오른쪽에 있는 일본어로 적힌 박스는 VJ 박스고고
아에 있는 사진은 VJ 커넥터에 홈을 파 가지고 라이트닝 커넥터를 끼워 넣었다 까지를 이해해야 됩니다
이미지를 정확히 이해를 해야 돼요 두 번째로는 기술을 이해하고 기술의
역사까지 알고 있어야 됩니다 그러니까 VJ 커넥트는 굉장히 낡은 커넥터 그래서 데이터 전송량이 훨씬 적고 그
대신 덩치는 크고 그리고 저 라이트닝 커넥터는 아이폰에 붙는 비교적 VJ 커넥터에 비하 훨씬
최신의 커넥터라는 기술 기술을 이해하고 기술의 역사를 이해해야 됩니다 세
번째로 이게 제일 무서운 건데 사람이 왜 웃는지 이해해야 됩니다 사람이
유머 감각에 대해서 이해를 해야지 저 질문에 대답을 알 수 있습니다 그 유머 감각을 알아채지
못하면 도무지 저게 왜 웃긴지를 설명할 방법이 없죠 바보가 이럴 수도 있잖아요 그러니까
그 세 가지를 다 하더라 인간의 유모 감각이라는 걸 GPT 4가 같게 된
것 아닌가라고 말하고 싶은 거예요 오픈 AI 쪽은 이것도 이제 이미지로
집어넣은 건데 프랑스 바칼로레아 물리시험 문제를 이미지로 집어넣고 이걸 풀어 달라고 하니까 얘가
프랑스로 된 물리시험 문제를 읽고 영어로 답을 할 하더라이 오픈 AI 쪽에서 내놓은
겁니다 근데 이제 오픈 AI 쪽에서 내놓은 증거 때에 언제나 이렇게 한다라고 믿으면 안 됩니다 이분들
연말에 인센티브도 받고 상금도 받고 해야 되기 때문에 가장 좋은 결과를
발표합니다 여러분도 그러지 않겠어요 하다 안 된 걸 왜 발표하겠습니다 보너스를 못 봤는데
그러니까 이런 발표를 할 때 항상 이렇게 반쯤 들어야 됩니다 아 이놈들이 인센티브를 노리고 있구나
트랜스포머를 쓰는 모든 거대 언어 모델은 반드시 네이션을 합니다
할루시네이션이 태어난 거짓말 멀쩡한 거짓말로 보내 갈 수 있을 텐데요 지역은 환각
있니다 그 미국의 응급 아과 전문의가 어떤 병에 관한 증상을 적고 채치
피트한 물어봤어요 이런 증상이 나타나면 이게 무슨 병일까 그랬더니 채치 피티가 대단히
정확하게 답을 했어요 그 병이 아니라면 그다음으로 가장 가까운 병은 뭐지라고 했더니 얘가 두 번째 병도
아주 정확하게 맞췄어요 그런데 그 뒤에다가 피임약을
먹으면 그런 증상이 나타날 수 있습니다라고 말을 딱 붙였는데 이게
거짓말이었던 거예요 헛 소리였어요 그래서이 의사가 35년 의사를 하신
분인데 너 그 근거가 있는 말이냐 나는 생전 처음 듣는데 했더니 얘가 아 근거가 있다 그래서 근거라면 돼
봐라 그랬더니 얘가 유로피언 jal of interal med이라고 이게 실제로 있는 권 있는
학술지다이 학술제 이름을 대고 제목을 대고 저자
박사 이름을 몇 개 대고 그다음에 European Journal interal 메디는 문서의 고유번호가 붙거든요 고유 번호까지 딱
써서이 사람한테 보여줬습니다 근데 그게 없는
논문이었다 거는 제가 이제 제비한테 물어본 건데 모자르트 첼로 협주곡을
알려 달라고 하니까 얘가 퀘 넘버까지 붙여서 다섯 고기나 저한테 알려 줬어요 그 퀘 넘버가 이제 닥터 퀘라
사람이 모자르트 고을 연대기 순으로 정리를 해서 번호를 붙인 겁니다 닥터
퀘이 붙였다고 해서 퀘 넘버라고 하는데 근데 모짜르트가 생전에 첼로 협주곡을 자곡 했는지는 모르겠지만
남아 있는 첼로 협주곡은 한국도 없습니다 제가 혹시 그 뒤에 뭐가 발
발견이 됐나 싶어 가지고 이거 차는데 20분 걸렸어요 거짓말입니다 왜 이런 짓을 할까요
얘가 무슨 부기 영화를 누리겠다고 사람한테이 거짓말을
할까요 아까 트랜스포머가 무슨 일을 하는 모델이라고 했죠 트랜스포머가 주어진
문장을 보고 그뒤에 올 가장 그럴법한 가장 근사한 가장 어프록시 미한
단어를 확률적으로 예측하는 일을 학습했다고 했잖아요 조교의 문서를 가지고 그러니까
얘는 제가 배운 대로 굉장히 성실하게 일을 한 겁니다 이게 정말 그럴 뜻
하려면 유 저널 오 인터널 메디 같이 권 있는 학술지를 돼야 되겠구나
그러면 정말 정말 근사할 거야 그리고 논문의 제목은이 정도는 돼야지 이런
증상에 관해서 얘기한 논문이 되겠지 그리고 의사 이름도 요렇게 적으면 정말 의사 이름 갔겠네 그 유피 전
인터 메디슨을 인용을 했으니까 당연히가 붙어야지 이렇게 해서 너무나
근사한 그럴듯한 답을 토해낸 거예요 그 뭐 저테 첼로 협지도 마찬가지죠
첼로 협주를 물으니까 첼로 협주곡이 아닐 수 없는 놀라운 제목을 적은
다음에 퀘 넘버를 붙인 거예요 그러니까이 친구는 자기가 학습한
그대로 최선을 다해서 결과를 내놓은 거죠 그러니까 모든 트랜스포머 모델은
할루시네이션 가질 수밖에 없습니다이 할루시네이션 대단히 조심하면서이 거대
언어 모델들을 써야 됩니다 이게 GPT 4가 나왔을 때 제가
나오자 말자 제가 쓸 수 있게 되자 말자 5분도 안 돼서 첫 번째 질문으로 즉시 할루시네이션 끄집어낸
겁니다 조선왕조 실록이 할루시네이션 a 보고입니다 이거에 관해서 질문을 잘하면 거짓말을 막
토해요 그 조선왕조 실록에 나와 있는 사도세자의 다리 부러진 사건에 대해서 얘기해 달라 저런 사건을
없거든요 그러니까 얘가 이제 쫙 거짓말을 하는 겁니다 막 세종 의
자 세종대왕이 4대 영조가 2니까 손자가 되기는 굉장히 어렵죠 저게 다
거짓말입니다 저기 나와 있는 오픈 a 쪽 발표로는 내버 테스트해 보니까 보다는 GPT 4가 19% 포인트 더
높은 점수를 받았다 그러니까 네이션을 88.6% 정도밖에 토해내지 않는다라고 얘기를 했어요 그러면
8.6% 마나 높은일까요 라는 개념이 있습니다
기계적 엄밀성이 개념이 있어요 그게 여러분들이 그 차를
모는데 88.6% 확률로 브레이크를 밟으면 액셀이 작동하고 악셀을 밟으면
브레이크가 작동을 한다고 쳐요 그럼 여러분들이 그차를 타시겠습니까
0.86 어땠어요 0.086이면 타시겠어요
기계적 엄밀성 그런 겁니다 브레이크를 밟았을 때 셀이 작동할 확률이 어야
돼요 액셀을 밟았을 때 브레이크가 작동할 확률이 0이어야 돼요 그걸 기계적 엄밀성이 말합니다 그러니까
8.6% 얼마나 높은 숫자인지 아시겠죠 얘를 믿고 뭔가 미션 크리티컬한 어떤 일도 하지 않는게
여러분들 신상이 좋습니다 그러니까 확률이 필요하지
않은 분야에 대해서는 얘한테 의존하는게 그렇게 현명한 일이 못 됩니다 그 다섯짜리 이상의 모든
덧셈과 뺄셈의 결과가 인터넷에서 올라와 있지 않죠 그러니까 25만 6천 자리하고 25만 4천 자리를
보텐 값이 인터넷에 계산 과정에 나와 있겠습니까 안 나와 있잖아요 왜냐면
우리는 더하기 100이 규칙을 아니까 그걸 올리는 미친 짓으로 할 사람도 없을 뿐더러 그걸 검색할 사람도
없겠죠 그걸 왜 검색을 합니까 계산기가 있는데 그러니까 트랜스포머는
확률적으로 예측을 하는 친구기 때문에 이런 일에는 어울리지가 않습니다 그리고 명확한 하나의 답이
있을 때 추측을 하라고 하는 것도 이상하지 요 그 피티가 굉장히 인기를
끄는 시점에 구글에서 구글이 바드를 두 번 발표하는데 초창기에 바디를 급히
발표를 해요 파인 튜닝을 하기도 전에 제대로 준비가 안 된 채로 발표를 합니다 우리도 이런 거
있다 그리고 주가가 200조가 날아갑니다 하루에 뭐냐면 제임스
스페이스 텔레에 관한 텔레스코 관한 질문을 하는데 이거는 명 하나의
사실이잖아요 이거에 대해 질문을 하니까 당연히 얘가 헛소리 했겠죠 그러니까 사람들이 채치 피티에 그
훌륭한 퍼포먼스에 열광을 하고 있는데 이런 똥볼을 찾으니까 사람들이 국을 망했다는 생각을 하겠죠 그래서 하루
아침에 200조가 날아갑니다 물론 지금 회복했습니다 예 이거 얼마
전에 있었던 일인데요 뉴욕 변호사가 채치 피티를 썼다가 5,000달러
벌금을냅니다이 사람이 제 어떤 에 대해서 변론 문을 쓰면서
채지 피티에가 이거와 비슷한 판례를 찾아 달라고 부탁을 해요 근데 미국은
불문법 이니까 앞에 나왔던 판례를 얼마나 잘 가져다 인용을 하느냐가 그
변호사의 실력이에요 그래서이 사람이 판례를 찾아 달라고 부탁을 한 거예요 그랬더니 얘가 금세
쫙 찾아 줬을 거 아닙니까이 변호사가 신중을 기하기 위해서 채집 피트한 너
이거 진짜야라고 물었어요 그랬더니 얘가 진짜다 대답을 했겠죠 당연히
그러겠죠 그래서이 사람이 그걸 인용을 해서 변론을 써서 법정에 제출을
합니다 근데이 사람이 인용한 판례 중에 여섯 개가이
세상에 없는 거였어요 원래 대로라면이 정도 죄를 저질렀으면 변호사 자격증을
뺏깁니다 법정을 못하고 사기를 쳤다 근데 판사가 제이 변호사한테 너는 바보니까
5천달러 벌만 기고 변증을 뺏지 않겠다 조심해라 이렇게 해서 바보여서
변호 증을 안 뺏기고 살았어 이런 식으로라 랭지 모델을 쓰는게 굉장히
잘못 쓰는이에요 이게 얼마 전에 행정안전부에서 공무원들한테 채치 PT
활용 방법 및 주위상 안내라고 뿌린 겁니다 전국의 공무원들한테 다 뿌렸는데 여기에 이제 공무원들을 골러
버리는 내용들이 다 들어가 있어서 제가 인용을 하는 겁니다 보시면 해외
사례를 매칭해 달라 이렇게 질문을 하면 좋다는 거예요 해외 연구 내용을 알려달라
이거랑 판례를 알려 달라 그다음에 공식적 자료만 활용해 달라 이거랑 너
이거 진짜지이 판례가 사실이냐 하고 공식 자료 활용해서 답해
달라 출처를 알려 달라 출처 알려 달라 그러면 뭐 이런
걸 말하겠죠 인라고 얘기를 하겠죠 그러니까이 행정안전부에서 공무원들을 다 골로
보내기 위해서 이런 걸 한 거죠 이거 보십시오 이게 지금 gbt
4가 저한테 한 답변이에요 그 포을 교육에 활용을 해 가지고
학습 효과를 높인 사례를 알려 달라고 했더니 이렇게 저한테 좋습니다
어때요 굉장히 그렇 보이지 않습니까 이거 다 가짜입니다이 정도로 훌륭하게 거짓말을
합니다 제가 이거 하는데 또 10분 넘게 걸렸잖아요 너무
그럴듯해서 근데 링크 눌러 보면 없는 페이지 입니다가
나오 그 이런 걸 하면 안 된다는 거예요 이렇게 하면 안 된다고요 해외
사례를 매칭해 달라 그게 이겁니다 그리고 이제 앞으로 여러분들이 그 API 말을 많이 들으실 거예요
그래서 API 아는 개념에 대해서 알고 있으면 좋습니다 API 애이션
프로그 인터페이스라는 건데 프로그램 간의 정해진 약속이에요 내가 발급한
API 통해서 나한테 요청을 하면 내가 정해진 포맷 대로 데이터를
주거나 혹은 정해진 대로 행동을 하겠다라는 거예요 그 예를 들어서
제가 이제 기상청에서 날시 데이터를 받아와서 제가 만든 솔루션으로 분석을 한
다음에 그 분석 결과를 필요하는 회사에다가 공급을 해 주고 먹고 사는 사람이라고 쳐요 그러면 이제 제가
기상청에서 데이터를 받아 와야 되겠죠 받아오는 방법이 여러 가지가 있을 겁니다 마다 가서 받아올 수도 있고
팩스로 받을 수도 있고 이메일로 받을 수도 있고 뭐 여러 가지로 받을 수 있겠죠 근데 어쨌든 이렇게 받으면
제가 이걸 처리하기 위해서 제 컴퓨터에 다시 쳐넣어야 될 겁니다 쳐넣어야 되죠 쳐려면 오타가
생길 수 있어요 그죠 뭐 스페이스바 잘못 누른다거나 3월 8로 누른다거나 뭐 숫자나
빼먹거나 뭐 여러가지 오류가 생길 수 있죠 그리고 시간도 많이 걸릴 겁니다 근데 기상 컴퓨터가 API
발급을 해요 그러면 제 컴퓨터가 그 API 가지고 기상 컴퓨터에 직접 연결해서 선생님 데이터 주세요 할 수
있겠죠 기상청 입장에서도이 API 발급을 하면 굉장히 좋은 점이 많습니다 우선 누가 요하는지 즉시 할
수 있어요 API 헤드에 내가 누구한테 발급 했다가 적혀 있을 거니까 그러면 얘한테는 어떤 데이터를
줘야 되는지도 적혀 있겠죠 그럼 걔한테는 그 데이터만 주면 돼요
그리고 끊고 싶으면 언든지 끊을 수 있죠 얘는 내일부터 주지 말자 그러요 API 블락해 버리면 되죠 그
리퀘스트에 다 적혀 있을 거니까 그리고 얼마만큼 나갔는지 어떤 빈도로
나갔는지 그러니까 만약에이 데이터를 유료로 파는 거면 자동 정산을 할 수
있어요로 요청할 경우에 그죠 그리고 제 입장에서도 굉장히
좋은 점이 많을 겁니다 일단 데이터가 자동으로 들어오면 데이터 발급에
걸리는 시간이 극단적으로 줄어들겠죠 그리고 제가 다시 쳐 놓 필요 없으니까 오류도 안 나올 거 그죠
거기다가 내가 스크립트를 짜 놓으면 기상청 데이터가 왔을 때 자동으로 내 솔루션이 그걸 분석하게 할 수 있겠죠
그다음에 분석 결과를 이메일로 보내는 것도 자동으로 스크립트를 짤 수 있을 거예요 그럼 어떤 일이 생기냐 기상청
컴퓨터가 API 발급해 주기만 하면 나는 제주도에 가서 낚시를 하고 있는데 매달 내통장으로 돈이 지키는
거예요 그게 API 지금 그 GPT API 발급했던 그래서
마이크로소프트가 오피스에 그 GPT 4를 연동하는데 성공을 했어요 그래서
내부적으로 지금 그 마이크로프트 직원들이 그걸 써보고 있는 중이라고 합니다 근데 얘기 들어봤더니
무시무시하게 잘 작동을 한대요 마이크로소프트 월드 엑셀 파워포인트가 GPT 4에 API 쓰면 어떤 일이
생기냐면 제가 이제 사장님이 뭔가 어떤 보고서를
쓰라고 했다 예요 그러면 제가 원래대로라면 브라우저를 열어서 포에
접속을 해서 어떤 질문을 던진 다음에 답이 나오면 그것을 복사해서 내
워드로 갖고 와야 되죠 근데 API 워드에 붙여
놨으면 제가 워드를 떠나지 않은 채로 이런 이런 보고서를 써야 되는데
목차를 뽑아줘 하면 그 목차가 워드 툭 떨어지겠죠 그죠 그 목차를 보고 근데
정말이 거대 언어 모델이 가장 잘하는 일 중에 하나가 목차 뽑는 겁니다 왜냐면 목차가 아주 뚜렷한
패턴이 있잖아요 이런 종류에 관한 보고서를 쓸 때는 이런 내용으로 써야지라는 패턴이 있단 말이에요 그런
보고서가 한두 개 있어요 세상에 엄청나게 많은 보고서
있을 거 아니에요 그중에 잘 쓴 보고서들은 목차가 되게 비슷할 거예요 학교의 그 강의 계획서 실라버스든지
보고서 목사 같은 거는 거대 델이 최고입니다 교수님들이 이제 막 다 공짜로 밥먹게
생겼어요 목차가 나오면요 그 목차를 보고 뭐 넣고 빼고 할 수 있겠죠
우리 회사에 맞게 그런 다음에이 목차를 다시
집어넣고이 목차의 내용을 채워줘 그러면 내용을 채워줍니다 주락
채워줘요 그러면 그 내용을 조금조금 고친 다음에 저장을 하면요 그
그다음날 쓴 것처럼 사면 드리면 돼요 그리고 회사에 서버에이 ap 적용을 했으면
얘가 우리 회사 데이터를 다 읽었을 거 아니에요 그럼 엑셀로 작업을
하다가 우리 회사 3년치 영업 이행률을 그래프로 그려 줘 하면 얘가 툭 그려 주는 겁니다 황당하죠 하여튼
그런 일이 일어나면 이게 그 마이크로소프트 오피스 한 달에 30달러 요금도 책정이 됐고요 좀
이따 공개가 될 겁니다 그러면 이제 뒤집어질 것 같아요 그 건너편에
플러그인이 있습니다 플러그인은 뭐냐면 그 API
반대로 채 GPT GPT 4가 다른 프로그램들의 API 불러와서 쓰는 겁니다 그러니까 다른
프로그램들을 체 GPT GPT 4가 도구처럼 쓸 수 있는 거예요 지금 뭐
700개가 붙어 있다고도 하고 2000개가 붙어 있다고는데 매주 계속 붙고 있습니다 엄청난
프로그램들이 붙고 있어요 울프 라는게 세계 최고의 수학 연산 엔진인데 얘가
미적분 수열 행렬 이런 걸 다 풀어주고 푸는 과정까지 보여줍니다 그리고 세계 최고의 여행
검색하는 엑스피디아 있고 뭐 오픈 테이블에서 레스토랑 예약하는 것도
있고 엄청나게 붙어 있어요 어떤 플러그인은 굉장히 뛰어난 역할 하고 어떤 플러그인은 뭐 그저 그렇고
이런데 그러니까 이렇게 플러그인이 GPT 4에 붙기 시작하면
어떤 일이 생기냐 그요 제가 이제 우리가 사인 가족인데 다다음
주에 교도에 일주일 동안 여행을 가기로 했다 그러면 GPT 4한 우리가 일주일 동안 다다음 주에
일주일 동안 교도에 여행을 갈 텐데 차를 렌트 하지 않을 거니까 대중교통으로 스케줄을 짜주고 금각사
은각사 청수사를 꼭 집어넣어 주고 일정 중에 이틀은 관을 이약을 하는데
값은 얼마에서 얼마 사인데 온천 굉장히 좋았으면 좋겠고 우리 막내딸이
비건이 레스토랑 예약은 전부 다 비건으로 해주라라고 오더를 하면 얘가 구글
맵을 열어서 동선을 짜고 xpd 열어서 검색을 하고 라운 테이블 열어서 레스토랑 비원을 차서 예약을
하고 한 다음에 리포트를 만들어서네 명한테 쫙 보낼 수
있어요 이렇게 되면 어떤 일이 생기냐 제가 제 뭐 를 나오고 제
친구들이 포항공대를 나오고 서울대 공국 갈 나오고 이런 사람들 아홉명이 모여 가지고 지난 2년 동안 인공지능
스타트업을 열심히 했다고 쳐요 그 바로 옆 사무실에는 인공지능과 아무
관계 없는 개발자 일곱 명에서 어떤 앱을 만들어서 서비스를 하고 있었어요 근데 갑자기 옆방 애들이
gpt4 API L 앱에다 붙인 거예요 그랬더니 갑자기 저 앱이
3.7조 원짜리 인공지능 앱이 돼 버렸어요 근데 나는 이제 석달만에 내
거도 나옵니다 내 것도 나오는데 나나와 본들 락이
없어요 내가 뭐 3.7조 원을 쓴 적도
없고 어떻게 해야 돼요 실제로 채집 피티가 나온 다음에
굉장히 많은 인공지능 스타트업들이 존재론적인 고민이 입사했어요 나와 본들 이게 파운데이션 모델을
그랬잖아요 거기다 API 나왔죠 그러니까 한 달에 몇십만 원
뭐 사용량에 따라 다르겠지만 몇 백만 원을 쓰면 이게 3.7조 원짜리 인공지능 앱이 되는
거예요 아 저도 한 15억 썼는데요 2년 동안 15억이 썼는데
어쩌라고 이렇게 되는 거죠 플러그인도 마찬가지입니다이
플러그인이 만약에 제대로 작동을 하게 되면 내가 내 이름을 걸고 내 브랜드를
걸고 열심히 싸우다가 말라 죽을 거냐 아니면 내일부터 오픈 AI 밑으로 들어가서
얘들이 떼주는 수술를 먹고 살아갈 거냐이 고민을 해야 되는 거예요
왜냐면이 밑에 플러그인으로 들어가는 순간 사람들이 내 브랜드를 잊어먹게 됩니다 왜냐면 gbt 4가 뭘 써서
나한테 그 서비스를 해 주는지를 내가 알 필요가 없잖아요 그리고 2000개가 넘으면 되고 어떻게 알겠어요 현 인공지능이 규모의
법칙을로 스케일을 갖고 있다고 했죠 컴퓨팅 파워를 많이 집어넣 많이 집어
넣을수록 학습 데이터를 크게 하면 크게 할수록 매개 변수를 많이 잡으면 많이 잡을수록 그냥 성능이 좋아진다
그랬잖아요 이게 무슨 뜻인가 하면 너는 00을 2,000자 정도 사서
클라우드에 붙일 수 있어 너는 학습 데이터를 5조 개 이상 가지고 올 수 있어 너는 매개 변수를 1조 8천억
개 이상 붙일 수 있어이 질문에 나라도 이해라고 대답을 못하는 순간
테이블에 앉지 못하는 거예요 그 그러니까 엄청난 자연 독과점 성격 가지죠
그런데 API 나와서 모든 앱에 붙이게 되고 제가
생각하건데 내년 말까지 앱스토에 올라가 있는 모든 앱에 90% 이상이 어떤 형태로든
API를 갖고 있을 겁니다 그러니까 모든 앱이 AI 하는 거죠 거기다
플러그인까지 만약에 제대로 작동을 한다면 근데 지금 플러그인 중에서 오픈 에가 직접 만든 플러그인들이
있습니다 그 브라우징 하는 플러그인이 있고 프라고 코딩하는 플러그인이 있고
브라고 해서 검색하는 플러그인이 있는데이 세 세 개는 정말 성능이
좋아요 그러니까 서드 파티들 거는 서비스에 가까워서 아직 시간이 좀
특별하지만 메인 펑션에 관한 오픈 a 만든 플러그인들은 무지막지하게 잘 작동합니다 그렇지 않아도 자연
독과점적인 성격이 굉장히 강한게 지금 인공지능인 이놈이 API 하고
플러그인까지 내놓고 있다 정말 무서운 시간이 오고 있구나
이렇게 생각할 수 있습니다 there 인공지능 버전 그 잠재된
패턴이 있는 모든 곳에서 인공지능이 굉장한 영향을 미치게 될 겁니다 피할 수 없어요 그리고 메타
페이스북 이름을 메타로 바꿨죠 페이스북에서 라라는 오픈소스를 내
얘는 그 매개 변수가 70억 개인데 150억 개인 GPT 3.5에 책
GPT 만먹는 성능을 보인다고 자랑하면서 나왔어요 어 이게 어떻게 가능하면 학습 데이터 양을
어마무지하게 늘렸어요 학습 데이터 향을 늘려서 훨씬 더 오래 학습을 시켰어요 그 긴 시간으로 학습을
시키니까 그 채집 PT 동급의 성능을 보이더라는 건데 사실은 동급의 성능은
아닙니다 테스트해보면 꽤 떨어져요 특히 할루시네이션이 굉장히 심하다고 그래요 그렇지만 트랜스포머 자체의
역량으로 보면 꽤한다 그럴듯한 말을 상당히 잘 지어낸다까지는 사실인 거 같고요 그
스탠포드 대학에서 라마 기반으로 알파카는 더 작은 모델을 또 내놓는데 얘는 학습시키는 시간이 정말 짧게
든다라고 자랑을 해요 이게 노무 인기를 끌어서 그 뒤에 이제
인공지능의 백화 제방의 시대가 되거든요 얘를 베이스로 해서 온갖 것들이 다 튀어나옵니다
그리고 그런 그 발전에 고무가 돼서 메타에서 얼마 전에 라마 2라고
라마보승이 더 좋은 모델을 7b 13b 70b 뭐
이렇게 7b 하여튼 뭐 세 가지 사이즈로 내놓습니다 상용으로서도 좋다 이까지
내놔요 그러니까 이제는 어떻게 돼 버렸냐면 어떤 독재자라
어떤 악덕 기업이라도 마음만 먹으면 그 라마투 정도의 고성능
인공지능은 다 집안에 몇 채씩 가질 수 있게 돼 버린 거예요 그니까 백화
제방의 시대가 열려 버렸고 보물이 터져 버린 거죠 원래 이거는 애초에
연구용으로 만 쓰라고 내놨는데 그 그럴이가 있겠습니까 인간이 온갖 걸
다 하는 거죠 여러분들이 중학교 사망년 때 술 먹은 것처럼 그렇게 하는 겁니다
그러니까 이제 게리 마커스를 비롯해서 여러 AI 과학자들이 우려를 표하기
시작했어요 그 얼마 전에 제 오픈 에어의 샘 알트만 그다음에 구글에
딥마인드의 데미스 서비스 그다음 마이크로소프트의 빌게이츠 그다음에 딥러닝을 만드신 제프 인턴을 비롯해서
글로드 만든 엔트로피의 CEO 아지 지금 거대 언어 모델의 유력한
플레이어들이 전부 다 합의해서 한 줄에 성명을 내는데 서명을 합니다 그
성명이 뭐냐면 지금의 인공지능은 팬데믹 그러니까 세계적인 유행병이
핵전쟁만큼이나 인류를 절멸시킬 수 있는 위험한 존재라는 것을 인식하고
다뤄야 한다라는 성명에 합의를 해요 그래서 공동 성명을 내놓습니다 그리고
그 성명으로 끝나는게 아니고 국제 기구를 어떻게 만들어야지 되느냐이
인공지능을 규율하기 위해서 어떤 국제가 필요한가를 또이 분들이 다
스탠포드 토론토 옥스포드 뭐 이런 대학들고 같이 손을 잡고 공동 저서로
논문을 내놓기도 합니다 왜 국제기 필요한가에 대해서 여러 가지로 설명을 하는데 이분들 꽤
진정성이 있다고 느껴지는 부분이 있는게 인공지능이 적용이 됐을 때
가장 큰 효과를 볼 수 있을 그런 지역과 나라에 정작 인공지능에 관한
인프라도 자원도 교육도 없는 경우가 있다 그렇죠 두 번째로는
그 이게 지정학적인 이익이 크기 때문에 나라간에 서로 다른 입장을 취할
가능성이 있어서 인공지능의 발전을 오히려 저해할 수가 있다 그리고 전
세계를에 대해서 인공지 이렇게 써야 된다는 규칙을 만들고 그 규칙을
지키는지 감시할 수 있는 그런 국제유가 필요하다 그리고 이 인공지능 안전에
대해서 전세계 과학자들이 함께 토론할 수 있는 그런 장이 필요하다 이렇게네 가지 이유를 들어서 그런 국제 기구를
만들어야 한다는 논문을 발표를 해요 그러니까 성명을 내놓는데 그치지 않고요 그리고 얼마 전에
구글하고 [박수] 오픈하고 페이스북 메하 엔트로피
하고네 개네 개가 티어는 포럼을 만듭니다 티어가 뭐냐면 지금의 언어
모델보다 훨씬 큰 더 큰 거대 언어 모델을이 사람들이 프론티어 AI 아고
불러요 그래서 프론티어 AI 포럼을 만들고 이거보다 더 큰 인공지능을
만들 때 우리가 합의해서 규율을 가지고 비윤리적인 일이 안 생길 수 있도록
노력하겠다라고 포럼을 만들기도 합니다 그러니까 인공지능이 가지는 위험이
그만큼 현실적이고 크단 얘기죠 그 게리 마커스가 이렇게 다섯 가지
위험성이 있다고 얘기를 해요 첫 번째가 극단주의자들이 어마어마한 허위
정보를 생성해서 민주주의와 공론을 쓸어 버릴거다 이게 무슨 뜻인가 하면요 그 최 gptn GPT 4를
쓰면 가짜 뉴스를 굉장히 쉽게 만들 수 있어요 누구누구가 뭐뭐를 했다는
뉴스를 써 줘 그러면 즉시 써 줄 거 아닙니까 트랜스포머를 써서 너무나 근사하게 써 줄 거란
말이에요 그렇게 해서 가짜 뉴스를 천개를 만들어요 천개를 만든다고 해봐야 시간이 얼마나 걸리겠죠 그러면
그 천개를 한번 뿌려봅니다 뿌려보면 그 천개 중에 어떤 뉴스는
굉장히 공유가 많이 되고 어떤 뉴스는 별로 공유가 안 되고 그럴 거 아니에요 그러면 천개 중에서 가장
공유가 많이 된 것 다섯 개만 뽑습니다 그러니까 5분의 5에
압도적인 성능을 가진 가짜 뉴스가 나온 거죠 그리고 얘를 카카오톡으로
미친듯이 뿌리는 거예요 그 어떻게 되겠습니까 막 할배 할매들이 정신을
차아 보니까 태극기 들고 광화문에서 있는 거죠 막 부들부들 떨면서 그래가
막 고함을 지르다가 집으로 돌아가는 길에 카톡을 열었더니 또 막 너무나 놀라운 뉴스가 있어서 또 차를 돌려
가지고 또 광에 가서 태극기를 들고 부들부들 떨다가 집으로 돌아가서 물 한잔 마시고 카톡을 열었더니 또
부들부들 떨고 뭐 이렇게 되는 거예요 이전까지는이 정도의 가짜 뉴스를 만들기 위해서는 굉장히 머리가 조고
교활한 사람들이 아주 애를 써야지 한 달에 한 10개 20개를 겨우 만들었을 거예요 근데
지금은 개든 2천 개든 만들어서 한번 뿌려본 다음에
그중에서 굉장히 뜨거운 반응을 보이는 그러니까 상 0.1% 상
0.01% 압도적인 성능을 가진 가짜 뉴스만 뽑아내서 그걸 카카 타오에게 미친듯이 뿌릴 수가 있는
거예요 엄청난 일이죠 그 환각이 잘못된 오히려 정보를 생할거다 이거는
아까 말씀을 드렸고요 컨텐트 팜이란게 검색 엔진에 알고리즘을 이용해
가지고 클릭을 받아서 광고 수익으로 먹고 사는 그런 것들을 컨텐트 팜이 해요 컨텐트 팜이 굉장히 자극적인
내용들을 아주 많이 만들 수가 있습니다 그러니까 아침에 많이 본 뉴스가 쭉 뜰 거 아니에요 그러 출근
시간 때 많이 본 뉴스가 쭉 뜨면 마리본 뉴스의 키워드를 쭉 끄집어내
가지고 보고 만들어 달라라고 하면 히 것들을 천개도 만개도 만들어 주겠죠
그럼이 친구들이 보유한 블로그니 카페니 온갖 군데다 이걸 막 집어넣는 겁니다 그럼 여러분들이 출근을 하다가
재밌는 뉴스를 봤어요 그러면 그 내용을 조금 더 알기 위해서 검색을 하면 26개의
진짜와 25,000개의 가짜가 섞여 있는데 검색 엔진이 그중에서 딱
26개만 골라서 여러분들한테 보여줄 확률이 얼마 될 것 같습니까 그 대환장파티가 일어나는 거죠 실제로
키에 대선이 가짜 뉴스로 판가름이 났다고 하지 않습니까 막판에 그래서
알고 있어야 될 위험들에 대해서 이제 설명을 드리겠습니다