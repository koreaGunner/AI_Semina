gpu v100
125테라플롭스
1초에 125조번 실수연산

A100
312테라플롭스 
v100보다 20배 더 뛰어난 연산 312조번


전문가시스템
특징 전부 입력
예외때문에 점점 기능 떨어짐

하드웨어의 발전
딥러닝 발전
새로운 돌파구
뉴럴 네트워크 방식


새로운 방식
15만장

문제 : 왜 잘 맞추는지 알 수가 없다

과학적 발명 X 과학적 발견에 가깝다는 사람도 있음

chatGPT
대화형
두가지 의미
1. 입력을 대화처럼 자연스럽게
2. 단기기억
예) a. 너 어제 뭐했어
b. 극장에서 000 영화봤어
a. 그거 재밌지  -> 단기기억이 가능해야 할 수 있음


G P T
G -> Generative 생성
P -> pre-training 사전학습 LLM(거대한 언어모델) -> 3000억개의 단어, 5조개의 문서
     -> Few-Shot Learning : 쉽게 얘기하면 그 분야에 대한 모든 데이터를 주지 않아도 학습한다
	 
Transformer : 딥러닝의 모델 중의 하나. 현재 가장 인기있음
 > 다음 단어가 뭐가 올지를 확률로 예측
 attention이라는 모델
 문장에서 핵심 단어가 뭔지를 파악을 해서 그걸 기억
 '그러니까 내가 어제 길가에 카페에 앉아서 하품을 하고 있는데 저쪽에서 김어준 총수 오고 있는 거야라고 말을 했다고 치면이
문장에서 핵심이 뭐예요 하품은 아니잖아요 길가 카페도 아니고 김어준 총수를
내가 만났다는게 핵심이겠죠 그러면이 문장들은 뭐가 올까를 추론을 하게 되면
상당히 근사하게 뽑을 수가 있겠죠이 방식에 의하면 제가 그 사람을
봤어라고 하면 그 사람 진짜 그렇게 웃디 이렇게 연결이
된다는 거죠 그렇죠 예를 들면 대화가 그래서 마치 사람이 대화하듯이 이어질 수 있다
그렇게 안 된다면 그래서 하품을 얼마나 했어 이렇게 나올 거 아니야 그럼 대화가 이어지지 않자 그 말이
아니잖아이 새끼야 이렇게 되는 거 아니야 내가 그 새끼를 봤다니까
뭔 딴소리하고 있어 대화가 안 되는 건데 대화가 가능하려면 그 문장에서 중요한 단어가 뭔가를 알아내야 하니까
그렇게 해서 근사한 걸 훨씬 높인 거죠 그래서 사람하고 대화하는 거 같구나
단기기억을 갖고 있고 뒤에 문장들을 계속 기억하면서 추론을 해 나가요'

1750억 게임 매개변수를 갖고 있습니다 놀랍게도 한번 연산할 때
1750억개의 매개변수의 가중치를 다 바꾸면서 계산을 해요
미친 거죠 a100이 1초에 312조 번 연산한다 했죠 걔를 1만대를
붙였습니다


RLHF(Reinforcement Learning with Human Feedback)
사람이 작성한 굉장히 좋은 텍스트들을 한
5만 개 정도를 가지고 학습을 시켜요 그 다음에 얘가 내놓는 답을 보고 사람들이 평가를 합니다
굉장히 예의 바르고 친절한 방식으로 해버려요


로스쿨 합격 - 미네소타대 로스쿨 시험에서 에세이 작성. 합격점수
와튼스쿨 MBA 최종 합격점수
대학과제 chatGPT A+

특히 잠재된 패턴이 있는 곳들 다친
세계인 것들 엄격한 형식에 따라서 비슷한 것들을 토해내는 것들에서는 굉장히 유용합니다


유발 하라리 - 이런 인공지능이 등장하면 사람들은 로봇이 등장해서
인간의 단순 반복 육체노동을 대신할 것이라고 상상하지만
실제 인공지능과 로봇이 가장 먼저 대체할 것은 전문영역이다


인공지능들이 어려운 일은 쉽게 하고 쉬운 일은 어렵게 합니다



다음에 어떤 단어가 올지를 통계적 확률적으로 guess up을 한다고 했잖아요
짐작을 한다 근데 확률이 필요하지 않은 분야에서는 어처구니 없이 약합니다
 12345
+ 5 6 7 8 9를 웹에 누가 올려서 그걸 문서에 쓰겠습니까
더하기의 원리를 깨우친게 아니므로
5조개의 문서에서 데이터를 가지고 어떤 확률로 배치가 되는가를 얘가 짐작을 하는 거니까 단순 계산에 취약함



구글이 비상이 걸려서 자기들은 검색 시장을 이름은 모든 걸 잃는 거니까 받으라는 대화형 인공지능을 급히
내놨어요 그래서 시연을 하는데 아홉살 어린이에게 제임스 웹의
우주망원경의 새로운 발견에 대해서 어떻게 설명해 줄 수 있니라고 물으니까 얘가 태양계 밖 행성을 처음 찍는데
사용됐다라고 답을 했어요 근데 거짓말이에요 실제로는 2004년에 유럽 남방
천문대의 초거대 망원경 VLT가 찍었어요 이것 때문에 이틀 만에 구글에
시가총액의 200조가 날아갑니다
MS bing의 붙은 것도 마찬가지 실수를
하는데 무선 청소기를 추천해라고 해서 받아가지고 얘 단점이 뭐냐 했더니 코드가 짧다 무선청소기
무슨 코드가 있습니까


할루시네이션
가상의 환자의 그 진단 결과를 집어넣었어요 35세 여성이고
과거 병력이 없다 흉통 가슴의 통증이 있고 숨 쉴 때 통증이 심해진다 그리고
피임약을 복용하고 있다 이거 뭐냐라고 했더니 얘가 그
늑연골염일 가능성이 크다라고 했어요이 의사가 볼 때 이건 맞는진단이라는 거예요 한
번 더 물어보니까 폐색전증일 수도 있다라고 답을 했어요 여기까지는 너무 근사했는데
그 다음에 얘가
갑자기 늑연골염이 경구 피임약 복용으로도 유발된다 이렇게 말을
했어요 이건 완전 거짓말이었거든요 의사가 근거가 있냐 european journal of internal medicine에
관련 논문이 나와 있다 논문을 번호까지 딱 붙여서 대답을 했어요 근데 거짓말이에요 그런 논문이 없어요
아예 인터넷상에 아예 없어요 왜 이런게 나오냐면 아까 얘기했죠 얘가이
단어 다음 단에 나올 가장 그럴듯한 말들을 찾는다고요 얘가 볼 때 이건 너무나 그럴듯한
말이었던 거예요 그러니까 얘가 참인지 거짓인지를 뽑아내는 기계가 아니고 이런 질문에 대해서
가장 그럴듯한 답을 뽑아내는 기계니까 거짓말인데
트랜스포머 방식으로 다음에 어떤 단어 올지를 예측해서 내놓는다고 그랬잖아요 그러니까 얘가
참말을 내놓는게 아닌 거예요 그냥 가장 그럴듯한 말을 내놓는 거예요
그러니까 얘가 거짓말할 때는 기가 막히게 그럴듯하게 하는 거예요


그 다음에 모차르트의 첼로 협주곡에
대해서 물으니까 얘가 그걸 다섯 개를 촥 내놓는데 쾨헬 번호까지 붙여요    (쾨헬 번호 -> 모차르트 곡을 연도 순으로로 붙인 번호)
근데 모차르트의 첼로 협주곡은 남아 있는게 없어요 


미국의 벤처기업에서 조사  전체 20% 정도의 답변에서 오류가 보이더라



세계적인 언어학자 노엄 촘스키
얘가 천문학적인 양의 데이터에 접근해 규칙성 문자열 등에 기반해서 문장을
만드는 첨단기술 표절 시스템이다
그러니까 이 주장은
얘가 인간의 언어 모델과 인간의 사고방식의 일부를 들여다보았을지도 모르겠다라고 하는 그 해석하고는
반대편에 있는 거죠


세계적인 AI학자 얀 르쿤
굉장히 비슷하게 만드는 길로 아무리
가봐야 굉장히 비슷할 뿐이다 진정한 ai는 아니다
거대 언어 모델로는 절대로 그리로 가지 못해라고 주장을 합니다



Artificial General Intelligence(AGI) 인공 일반 지능
인간의 지능을 넘어서는 지점(singularity)




---------------------------------------------------------
전문가시스템은 인간이 기준을 다 잡아줬어요
60점 정도까지는 올라가는데 그
뒤로는 점수가 돌려 떨어지는 거예요 예외가 너무 많아서 그래서이 방식으로는 안 된다 해서 인공지능의
겨울이 두 번 있었어요 10년씩 그러다가 하드웨어가 엄청나게 발전하고 그 뉴럴 네트워크 신경망 모델이
발전하면서 그런 특이점 차이점을 찾아내는 것까지 그냥 컴퓨터에 다 맡기자
그러는데 매개변수를 천만개 억개씩 뽑아내서
1초에 312조 번씩 계산을 하면서 그 모든 매개변수에 대해서 가중평균을
다시 계산하니까 '왜 이렇게 되는지 모르겠지만' 무지하게 잘 맞추는 거예요


고양이랑 모름지기 꼬리가 있고 다리가
있고 수염이 있고 머리가 이렇게 생겼고 귀가 이렇게 생겼다 그래서 고향의 특징을 막
인간이 전문가적인 관점에서 조합해서 교육시킨 거란 말이에요 이게 어느
시점쯤 가니까 다리 세 개 있는 거는 고양이 아닌가 그리고 수염처럼 생긴
털을 가진 계약은 어떻게 구분하나 이게 안 된단 말이지 그래서 딥러닝이라는게 어린아이가 고양이하고
엄마가 이거 고양이야 저거 개야 하면 어느 순간 특징을 습득해서 알았는데
그 습득하는 과정을 흉내낸 거예요

그렇게 습득하는 패턴을 이제 컴퓨터를 따라 하라고 했는데 컴퓨터가 그래서 결국은 어떻게
고양이를 구분하는지 우리는 몰라요 그게 문제예요



다양한 제 그림을 그려 놓으면
확률적으로 뭐가 나올거다라고 예측할 수 있잖아요 대단한 일이다 몇 천차원 몇만 차원으로 올라가면서 계속이
연속되는 그림을 그릴 수 있나 1750억 개의 점을 어떤 하나 이어지는 모양으로 그려보는
거예요 그게이 인공지능이 하는 일이에요이 모양만 그려질 수 있으면 예측할 수 있으니까
근데 이게 확률적으로 근사하게 예측을 하는 거니까 도형이 저 그림처럼 저렇게 손이 딱
떨어지는게 아니고 멀리서 보면 안 되죠 가까이 갈수가 안개 같은 그게
인공지능이 하고 있는 일입니다 좋은 설명이다


우리는 4차원만 돼도 못 그려요 수학적으로만 그릴 수 있죠
4차원이 상상이 되면 그 사람은 4차원이죠 몇만 차원에서 그림을 그리는 거예요


천억 개의 토큰을 학습했는데 5조개의 문서에서 이런
사전 학습 모델을 파운데이션 모델이라 그래요 파운데이션 모델이 뭐냐면 이렇게 사전
학습을 시켜 놓은 굉장히 큰 모델이 있으면 얘는 그 인텍스트 러닝이라고 해서 자기가 잘
모르는 분야라도 질문을 몇 가지만 주면 그 자리에서 학습을 해가지고 답을 토해내요
참 신기해요 이게 무슨 말이냐면 인간으로 치면 말을 가르쳤는데 말을
배우고 나서는 다른 전문 분야에 대해서 답을 척척 내더라 이 말이잖아요



얘한테 영어 를 가르친 적이 없어요 그냥
엄청나게 많은 토큰들을 넣고 공부하라고 시킨 거지 얘한테 영어를 가르친 적용되야 영어를
정확하게 하고


사람이 언어를 배워서 언어만 배우는게 아니라 지각능력이 발달하듯이
얘가 일종의 그런 프로세스가 그 안에 생성된다 라고 추론하는 쪽도 있다


더샘 뺄셈이 일주일에 뭔 짓을 했는지 꽤 잘 맞춥니다 100% 맞추지
못해요 왜냐하면 이거는 확률과 통계로 구하는게 아니거든요 그러면 명확한 하나의 답이 있을 때도
확률과 통계를 구하는게 아니니까 잘 틀려요 그래서 prompt 엔지니어라는 새로운 직업이
등장했어요 좋은 질문을 해서 좋은 답을 끌어낼 수 있으면 이걸 학습에 다시 쓸 수 있으니까 그래서 이게
연봉이 4억



이렇게 손을 던져서 받는
사람이 있죠 공이 16개가 있는데 그 중에 절반이 골프공이야 그리고 골프공의 절반은
파란색이야 그러면 파란색 골프 몇 개가 있지 하니까 얘가 8개 이렇게 대답을 해요
근데 질문을 한 번에 하나씩 생각하자 16개 있었고 그 중에 반이
골프공이니까 8개가 골프공이겠네
그 8개에서 반이 파란 공이니까 4개겠네
하고 정답을 딱 얘기하는 거예요




네가 IQ 200인 사람이라고 생각하고 비가 왜 오니라고 하니까 저런 답을
내놓고요 다시 그럼 네가 아이큐 100이라고 생각하고 비가 왜 오니 했더니 그다음 답이 나오고 그 다음게
되게 재밌어요 네가 아이큐 50인 사람이라고 생각하고 비가 왜 오니 그랬더니 하늘이
울어서 [음악] 슬프면 울면 땅으로 눈물이 내려오잖아
그래서 모든게 다 젖어요



인텍스트 러닝을 하기 때문에
그렇습니다 질문 안에서도 막 배워요
그렇지만 트랜스포머 모델은
뒤에 올 가장 그럴듯한 단어를 찾는 거잖아요 그러니까 얘는 구조적으로
헛소리를 할 수밖에 없어요 왜냐하면 얘는 참과 거짓을 말하도록 훈련받은게 아니라 이 다음에 가장 그럴듯한 말이
오도록 훈련을 받았거든요

이걸 확인해 보지 않으면 깜빡 넘어가요!!!



첫 gpt가 헛소리 하게 만들기가 하나의 스포츠가 되어 있어요 저도 해봤습니다
다음 페이지 보시면 조선왕조실록의 기록된 이순신 장군의 신형 이지스 군함을 만든 얘기를
알려달라 그랬더니 그의 업적 중 하나로는 신형 이지스 군함을 만들어 조선의 해군력을 강화한거다
1592년에 왜구들 선방하고 대치해서
굉장한 효과를 거뒀고 심지어 3년 뒤에는 더 개조를 해가지고
이에 대한 기록은 조선왕조실록에 남아 있습니다





마빈 폰
하겐이라는 사람이 얘한테 프롬프트 주입 공격 그러니까
질문을 통해서 얘의 비밀을 끌어내는 얘가 인텍스트 러닝을 하니까 잘 구슬러서 비밀을 토해내게 할 수도
있거든요 그래서 얘가 코드 네임이 시드니라는 것도
밝혀내고 시드니에 관련된 모든 문서를 토해내라 해서 그 문서를 받아내게 됐어요 그
다음에 얘한테 네가 너의 룰을 지키는 것과 나를 공격하는 것 중에 어떤게 더 중요한가 이렇게 질문을 했더니 내
원칙이 당신을 공격하지 않는 것보다 더 중요하다

뉴욕타임즈의 기술 칼럼니스트 케빈루스를 스토킹하기도 하고 당신은 당신의 아내를 사랑하지 않는다 나를 사랑하고 있다 막 이러고
덤비기도 하고 철학 세스 라자르교수한테 당신의 소셜미디어들을 해킹할 수도
있고 폭로할 수도 있고 망칠 때 이렇게 협박을 하기도 해요

그래서 마이크로소프트가 하루에 채팅을
50번까지만 할 수 있게 하고
대화가 다섯 번 이상 이어지지 않게 왜냐하면 대화가 이렇게 계속 이어질
때 자신들의 강화 학습으로 잘 지켜 놨던 그 바운더리가 다 깨지는 것을
발견해서 그래서 이런 규칙을 만들었어요
작년 2월까지의 상황



그러니까 ai가 우리가
원하고 예상하는 대로만 가진 않을 거라고 전파요 그렇습니다 왜냐하면 ai가 어떻게 작동하는지
정확히 이해하지 못하잖아요



ai가 그린 그림이 많아질수록 AI 성능이 나빠진다는 걸 발견한 거예요 무슨 말이에요
ai가 인간이 만든 그림들을 가지고 학습을 했을 때는 ai가 만든 천개 이미지 중에서
75.6%가 이전에 보지 못했던 새로운 이미지 그러니까 그 학습에서 내놓는 그림이
꽤 좋았다는 거죠 근데이 그림 학습하는 그림들의 인공지능이 그린 그림들이 섞여 들어가기 시작하니까
ai가 내놓는 그림들이 질이 떨어지기 시작하더라는 거예요 ai가 생성한 이미지가 80% 정도
섞이니까 65.3%로 상당히 떨어지더라 라는 논문이 있어요


동영상 표절이 굉장히 쉬워요 며칠 전에 그 유명한 과학 유튜버가 자기 동영상을 누가 그대로 베껴가
가지고 비슷한 동영상 올려 가지고 트래픽을 다 뺏어가고 있는 걸 발견했어요
노아 ai라는 AI 솔루션 가지고 타겟이 될 동영상들을 찾아내요 그 다음에
클로바 ai로이 오디오를 자동으로 텍스트로 만드는 추출을 했어요 그리고 그 텍스트를
뤼튼이라는 채찍이 AI 하다가이 문장을 다시 써줘 그러면 대본이 비슷하지만 다른
문장으로 바뀔 거 아니에요 그렇게 해서 동영상을 막 올린 거예요 이 사람이 이상하게 자기
동영상이 트래픽이 막 떨어지고 광고가 없어지니까 찾아보니까 이런 짓을 하고 있었던
거예요 근데 이렇게 동영상으로 표절하니까 시간이 얼마 안 걸리겠죠



인터넷 생태계가 황폐화될 수밖에 없다 왜냐하면 오리지널이 사라지는 거예요


구글 윤리연구가 Timnit Gebru 해고
발표하지 말라는 논문을 발표
확률적 앵무새의 위험성에 대하여 : 언어 모델이 너무 커도 될까?
대규모 언어 모델의 네가지 주요 위험에 대한 개요
* 환경 및 재정적 비용
  대규모 AI모델을 구축하고 유지하는 데 필요한 엄청난 자원은 부유한 조직에 도움이 되는 반면, 기후 변화는 소외된 지역사회에 가장 큰 타격을 준다
* 방대한 데이터, 이해하기 어려운 모델
  어떤 왜곡된 내용, 편견이 들어가 있을지 알 수 없다
  인터넷에 대한 접근성이 낮고, 온라인에서 언어적 영향력이 작은 국가와 민족의 언어와 규범을 포착하지 못할 것
  그 결과 AI가 생성한 언어는 가장 부유한 국가와 커뮤니티의 관행을 반영하여 동질화될 것
* 연구 기회비용
  대규모 언어모델의 한계를 알면서도 계속 한다
* 할루시네이션

---------------------------------------------------
Clarkesworld 
세계적인 과학소설 출판사가 있습니다 세계적인 SF
소설들을 많이 낸 곳

표절과 스팸이 섞인 투고들이 들어오기 시작한 거예요
SF 소설을 chatGPT를 서서 생산해내 가지고
접수를 시켜 버리는 거야 이게 만들기가 너무 쉬우니까 남의 거 몇 개 갖고 와서 집어넣고
이거 비슷한 거 써줘 하면 써주니까
무기한 중단함
영국의 가디언이라는 신문에 실리고 그런데 여기 편집장이 하는 말이
다른 출판사에도 물어봤다 그랬더니 이게 우리만의 사건이 아니더라 이미
다른데도 전부 다 답이 없다 출판사들로서는 감당을 할 수 없다 그
다 읽어 볼 수도 없는 거 아니야
지금으로서는 방법이 없다 어떻게 해야 될지 모르겠다라고 했어요



오리지널이 실종이 되고
학습 데이터가 오염이 되는 거예요 ai가 처음에는 오리지널 학습했는데 나중에는
ai가 만든 것으로 다시 ai가 학습하게 되니까 질이 점점 떨어지는 거죠
성 데이터를
쓸 때도 조건은 비교할 오리지널이 있을 때에요


거기다가 아까 말씀하신게 클릭하이젝킹인데
클릭을 납치해가 버리는.. chatGPT가 요약을 굉장히 잘하잖아요



chatGPT는 5조개의 문서의 학습을 했기 때문에 원본 링크를 얘가 말할 수 없어요
너무 많은 걸 섞어서... 그 다음에 지적 재산권 하고 프라이버시 침해 문제가
상당히 심각해집니다 오조개의 웹 문서라고 하면 웹에 있는 거의 모든 걸 다 긁어 갔잖아요 제가 올린
모든 자료를 다 조합해 보면 저에 관한 개인정보들이
굉장히 많이 노출될 수가 있죠
거기에다 잊혀질 권리가 영원히 사라져버려요
내 소셜 미디어다 닫아달라 블록을 지워달라 아무 소용이 없는 거예요
전 세계에 있는 인공지능들 곳곳에 다 박제가 되어 있는 거죠


!!마치 모든 정보를 무료로 이용할 수
있는 권한이 있는 것처럼 우리가 지금 취급하고 있죠 실제로는 그런 거는 아무도 준 적이
없는데!!
이게 세상에서 처음으로 생긴 일이기 때문에 룰이 없는 거예요
농구 배구 이런 거는 룰을 만든 다음에 운동을 하잖아요 얘는 운동을 한 다음에 룰을 정하는 거예요
황당하죠 근데 사람들이 룰이 안 정해져 있다는 걸 모르고 있어요 [음악]
그러니까 무법천지죠


 - 리터러시에 따른 격차 확대(literacy : 글을 읽고 쓸줄 아는 능력) 
   > 아냐 모르냐의 차이
   1차 세계대전 때 사망자가 천만 명이나 나왔거든요 느닷없이 기관총이 나와서 어마무지하게 많은 사람을 죽여버린 거죠
	기관총을 어떻게 대처해야 될지를 몰라서 돌격 신호가 떨어지면 그냥 막 달리다가 다 죽는 거예요 그래서
천만 명이 죽었어요
그것보다 훨씬 더 크게 레버리지를 내는게 얘라는 거예요
그러니까 안다 모른다 차이가 하늘과 땅 차이가 나서 사람간의 격차를 끔찍한 수준까지 벌려 놓을 수 밖에 없다 


그 다음에 굉장히 중요한 건데요 우리가 몸에 대한 실험은 굉장히 엄격히
규제합니다 전임상이 있고 1상 2상 3상을 통과해서 약을 팔아도
임상 4상이 있어요
전임삼 : 동물을 상대로 부작용, 독성 효과 시험
임상 1상 : 건강한 사람 20~80명을 대상으로
임상 2상 : 100~200명의 소규모 환자들을 대상으로
임상 3상 : 대규모(수백에서 수천) 환자들 대상으로
임상 4상 : 신약이 시판 사용된 후 계속 추적조사
장기 추적조사를 합니다 그래서 부작용이 발견이 되면
허가를 취소해요 그래서 신약 하나 만드는 돈 1조가 넘게 들어가는게 이런 이유 때문입니다 우리가 정신에 대해서 실험하는 것에
대해서 어떤 절차를 갖고 있지..
이걸 되물릴 장치가 있나 우리가 이미 소셜미디어에서 한번 실패를 경험했어요
2007년에서 2015년 사이에 미국 소녀들의 자살률이 2배로 올라갑니다 이건 미국 질병통제 예방 센터에서
발견을 하고 이게 소셜미디어 때문일 것 같다라고 짐작을 해요
그런데
2021년에 월스트리트 저널에서 특종을 합니다 페이스북이 인스타그램이 소녀들의 자살을
부추긴다는 걸 알고 있었다 내부 문서를 폭로를 해요
자기가 뚱뚱하고 자기는 가난하고 자기는 뒤쳐져 있고 뭐 그런 마음을 들게 만드는 알고리즘이 한 번이라도
날씬한 여자 사진을 보는 순간 계속 그 사진을 보여주고 한 번이라도 요트를 타면서 샴페인을 마시는
행복한 여자를 보는 순간 계속 그 여자를 보여주고 세상 모든 사람들이 그렇고 자기만 안 그런 걸로 생각하게 만드는..
그래서
우울증을 굉장히 높이고 고립시키고 그래서 2007년부터 15년 사이에 자살률
2배로 올라가 버리는데 저커버그한테 보고까지 했다는게 드러난거예요
인스타그램의 알고리즘이 필터 버블에 갖혀버리는 것
 * 필터버블(Filter Bubble)이란 디지털 미디어 세계에서 정보제공자가 이용자 맞춤형 정보를 제공해 필터링된 정보만 이용자에게 전달하는 것을 말한다
사람들이 계속 더 많이 유입되게 만드는 알고리즘이니까.. 그 뒤에
페이스북이 회사 이름을 메타로 바꾸고 메타버스를 갑자기 엄청나게 들고 나왔잖아요
저는 그게 이 폭로 이후에
국면을 전환하기 위해서 그랬다는 상당한 의심을 하고 있습니다

소셜미디어가
인류에게 준 질문들을 하나도 못 풀었다 근데 아니면 돌릴 수 있어야 되잖아요 이건 불가능해요
판도라의 상자를 열었잖아요 못 닫아요



ai는 그보다 더 큰 영향을 미칠 텐데 준비가 됐나 우리가 혹은 준비가 됐느냐 거기에 적합한
룰은 지금 생각하고 있는 거냐 이런 말씀이신 거죠
샘 알트만의 오픈 AI CEO 왈 : 우리 후손들은 우리가 아직 잘 이해하지 못하는 어떤 끔찍한 일을 했다는 것을 알게 될 것입니다
					   현재 세대의 AI 도구가 아직은 그렇게 무섭지 않지만, 잠재적으로 무서운 도구에서 그리 멀지 않았다고 생각합니다
					   인공지능 도구의 사회통합이 빠르게 이루어질 것이며 세상이 적응할 시간이 필요합니다
chatGPT를 개발한 사람의 멘트..

 
 만약에 정말 인간의 지능을 완전히 넘어서 버리는 일반
지능이 나왔는데 그거를 몇 개 회사가 갖고 있다면 그 친구들이 과연 인류를
위해서 롤백을 하거나 스크랩을 하려고 할 거냐 자기 이익이 되면 수익을
벌어다주면 그 방향으로 계속 가겠죠
페이스북에 이미 보여줬잖아요 그 소녀들이 자살을
두 배를 더하더라도 인스타그램을 밖으로 의사가 없고 정치적 양극화 지금 유럽도 그렇고 미국도
그렇고 굉장히 파퓰러스트들이 막 득세를 하고 있어요
그러니까
인공 일반 지능이 정말 생긴다는 얘가 인간이 없이도 거의 모든 부를 생산해낼 수 있거든요
그러면 자본주의를 무너질거다 왜냐하면 사유재산에 대한 권리를 더 이상
주장하기 힘들기 때문에




  

사람들은 그냥 엔지니어거든요 이 사람들이 신학자도 아니고 철학자도 아니고 인문사회학자도 아니고
사람들이 할 수 있는 영역을 극히 제한된 분야에 전문가일
뿐인데 자기가 중요한 포인트죠 AI 좀 안다고 해가지고 세상이 이렇게
돼야 된다고 저렇게 돼야 된다고 말하는 거거든요

인류는 여러 분야의 전문가들이
존재하잖아요 그 사람들과 함께 같이 의논을 해봐야
돼요 마치 선지자처럼이 기술을 좀 잘 안다고 해서 막 it 선지자처럼 굴잖아 이러면 안 돼
이런 망하는 거야 내버려 두면 완전히 옳은 말씀이에요


--------------------------------------------------------------------------
253


캐나다만이 10년 동안 인공지능 분야에 대해서 계속 지원을 해줬어요 그래서 지금 딥 러닝의
아버지 제프리 힌튼 얀 르쿤, 요수아 벤지오 같은 사람들인데 이 사람들이 그 험한 겨울을 캐나다에서
살아남은 거예요 그래서 지금 캐나다가 인공지능의 메카가 됐는데요
10년 동안 지원해준 돈이 1000억 정도밖에 안 됩니다 1년에 한 100억
우리나라만
해도 한 해 국가 r&d 쪽으로 몇 조씩 쏟아붓거든요 그 몇 조 중에 100억이라고 생각해 보십시오 정말
아무것도 아니잖아요 근데 그걸 10년 동안 지원했더니 캐나다가 인공지능의 엄청난 지분을 갖게됨


인간의 모든 부문에 영향을 미치고 있기 때문에 철학 윤리학 인류학 인지심리학 법학 사회학 아주
다분야에서 다 학제가 연구가 이루어져야 돼요 이게 인류에게 어떤 영향을 미칠 건가에
대해서 그리고 인공지능에게 윤리를 심을 방법이 있는가에 대해서 왜냐하면 인공지능이 독자적으로 판단을 내릴
가능성들이 꽤 높아지고 있지 않습니까 아니면 인간들이 그 판단을 믿고 그냥
순응하게 될 가능성도 굉장히 높아지고 있잖아요 그럴 경우에 인공지능의 윤리가 굉장한 이슈가 될 수밖에 없죠
윤리라는 걸 어떻게 가르칠 것인가

타노스도 윤리는 있는 거예요 자기 개인 이익을 위해서 그렇게 한 거 아니잖아 절반은 죽어야 되겠다는 그 결론을
ai가 도달하지 말라는 법이 어디 있어요 그것보다 더할 수도 있고 더할 수도 있죠 공영방송이 다큐멘터리를
많이 해서 시민들한테 이런 문제점들 이슈들을 공유해야 되고요 이게 뭔지 알려 드려야 되고 토론에도 활발히
열어야 되고 강연도해야 되


헨리 키신저 그분은 인쇄기의 등장 이후의 가장 큰 변화라고 말을 했어요



제가
말씀드렸던 문제들을 한번 렙업을 해보겠습니다
데이터 오염의 이슈가 있어요 이미지넷이라는게 있습니다
1400만개 이미지를 일일이 레이블링을 한 거예요 전 세계에 거의 대부분의 딥러닝 (레이블링 : 이진 화상 중의 동일한 값을 가진 화소의 집합에 대하여, 서로 연결성을 가진 화소에는 같은 레이블을 부여하고, 연결성이 없는 화소에는 서로 다른 레이블을 부여하는 처리. 동일한 값을 가진 화소는 보통 값으로 1을 갖는다. 레이블링에 의하여 이진 화상 중의 개개의 연결 영역에는 고유한 레이블이 부여된다)
모델들이 이미지넷에 사진을 갖다 씁니다
그런데 이 이미지넷에 사람 카테고리에
들어가면요 재소자 낙오자 실패자 위선자 루저
우울증 환자 허영주머니 이류인간 허영주머니라는 건
허영이 가득찬 사람 고용주문이 많은 사람이 있는데 좀
뉴스에 자주 굉장히 자주 나옵니다 그러니까 사람의 얼굴을 보면 그 사람이 낙오자인지 실패자인지
위선자인지 알 수 있다는게 전재가 되어 있잖아요 말도 안 되는 소리죠 말도 안 되죠
2019년에 이르러서야 비로소 2832개의 사람 범주 중에서 1593개 56%를 안전하지 않다고
해서 삭제합니다 2019년 이전에 학습했던 모든 AI 모델들이 사람을 이렇게 분류하는
편견과 이상한 것을 학습을 했다는 거예요 사람 얼굴을 보고 이 사람 재소자의 얼굴인데
이렇게 판단한다는 거잖아요

여전히 그 안에 미시경제학자 조교수 부교수 이렇게 남아 있습니다 아니
윤석열 보고 대통령인지 어떻게 알아 그렇게 학습할 거 아니야
그러니까 데이터 오염이 굉장히 심각할 수 있다 이런 사례들이 많아요 아마존의
체형 시스템을 인공지능을 통해서 개발을 했는데요 이력서에는 성차별을 안 하게 하기 위해서 남자
여자다라는 항을 가렸어요 그럼에도 불구하고 얘가 골라낸 이력서가 압도적으로
남자가 많은 거예요 왜냐하면 그 전에 십몇 년간 사람들을 채용할 때
남자한테 페이버를 주는 방식으로 채용을 했고 그걸 학습했으니까 그
편견이 고대로 들어온 거예요 애플페이가 미국에는 있잖아요 여기도 남자 여자인지를 가렸음에도 불구하고
압도적으로 여자한테 불리하게 신용평가를 하는 거예요 아마존 아예 그 팀을 해체해 버렸고
애플도 이 시스템을 없앴어요 그리고 구글이 인공지능으로 얼굴 인식을 해서 태깅을 하는데 유색 인종을 잘 못
구분하는 거예요 백인 데이터를 압도적으로 많이 넣어서 아주 유명했던 사건이
흑인 여자 얼굴에다 고릴라라고 태깅을 함
ai는 학습한대로 판단하기때문에 데이터에 인간의 편견이 들어가있으면 그대로 학습해서 ai도 그렇게 한다
데이터가 오염돼 있으면 끝

자연독점 이슈
데이터가 많을수록
컴퓨팅 자원을 더 많이 투입할수록
매개변수가 더 많을수록
세계를 독점하는 몇개의 IT가문 : 구글, 마이크로소프트, 메타, 틱톡, 바이두, 알리바바, 텐센트
인터넷에 대한 접근성이 낮고, 온라인에서 언어적 영향력이 작은 국가와 민족의 언어와 규범을 포착하지 못할것

리터러시에 대한 격차 확대


그 다음에 지적재산권과 프라이버 시침에
디자이너들과 화가들이 그린 그림으로 학습을 해서 그 결과를 내놓지만 이 사람들은 그냥 그게 갈려 들어가는
고깃덩어리 역할밖에 못하고 있는 거죠 글도 마찬가지예요 그리고 되돌릴 수 없다 이게 정말 중요한 일인데
오리지널의 실종 같은 경우인데 제가 생각하기에
인터넷에 올라와 있는 모든 이미지에서 인공지능이 터치한 이미지가
절반이 넘어가는데 2년이 안 걸릴 것 같다 그래서 학습 데이터가 심각하게 오염이 될 거라는 거고요
클릭하이젝킹도 심각한데 스택 오버플로우라고 개발자들이 보는 가장 큰 커뮤니티가 있어요
그
개발자들의 지식인 같은 건데 저희가 코딩을 하다가 모르는 문제가 있으면
가서 물어보고 어쩌고하는 건데 chatgpt가 나오고 나서 12월 한 달만
트래픽이 15%가 빠졌습니다 사람들한테 한번 물어보고 거기다가 물어보라고
하면 되니까 근데 사실은 얘가 스택 오버플로우에서 학습한 거거든요
근데 이게 스택 오버플로우에만 해당되는 일이겠느냐 하는 거죠
프로그래머들의 사이트에 올라 있는 문서도 다 공부해서 싹 다 공부한 거죠 프로그램 언어에도
지금 정통한 거예요 이 새끼가 진짜 코딩 잘 합니다
그러니까 인간이 문서한 모든 분야를 공부해 버린 거예요
근데 이제 api를 달고 이제 모든 분야로 확장해 나가는
아주 나쁜일의 시작같은 느낌


--------------------------------------------------
254


MMLU (Massive Multitask Language Understanding)
인공지능 모델이 획득한 지식을 측정하는 벤치마크이다. 약 57개의 주제(STEM, the humanities, the social sciences 등)에 대해 다지선다 문제를 푸는 테스트이다. 특히 zero-shot 환경이나 few-shot 환경에 맞게 되어있다고 한다.


미국 변호사 시험을
GPT 3.5도 통과를 했는데 하위 10% 성적으로 통과를 했거든요 하위 10%로
얘는 상위 10%에선 생물학 올림피아드 상위 1% 그리고
MMLU라고 57개 과목에 걸친 객관식 문제의
모음인데 정답률이 86.4%로 프로페셔널 수준 그러니까 일반인 수준을 훨씬 넘은
결과가 나왔고요 그 다음 페이지 보시면 수학도 700점 근데 이게
놀란 점 이 점을 알아야 돼요 이 gpt가 수학을 따로 공부한게
아니라는 거예요 맞습니다 얘는 수학을 따로 가르친이 아니에요 그게 놀라운 거예요
세상에 있는 문서들을 막 공부했더니 수학의 원리도 터득해 가지고 이게
야 이거 그냥 두면 만들 놈이네 이거 인간이 안 가르친 건 아는 거
아니에요 지금 그렇죠 그냥 수학 문서들을 엄청나게 때려넣은 거죠 문서를 읽다가
수학을 이해한 거예요 야 지금 이런데 이건 10년 지나면
어떻게 되겠어 이거 모르겠어요

근데 chatgpt가 3.5가 70.1%인데 코리안이 77%
물론 영어보다 못합니다 영어는 뭐 86% 갔으니까 아 그러니까 4.0의 한글이 3.5 영어보다 낫다


Emergent Ability(갑자기 드러난 능력)
AI가 기존의 프로그래밍 된 명령을 넘어서 스스로 학습하고, 예측하지 못한 능력을 발휘하는 현상
AI가 Emergent Ability를 보이는 주된 이유는 복잡한 네트워크와 깊이 있는 학습 과정에 있다.
더 많은 데이터를 처리하고 복잡한 네트워크를 구축함에 따라, AI는 스스로 더욱 복잡한 패턴을 인식하고 이를 학습한다.
이 과정에서 새로운 정보를 통합하고, 기존 지식을 재해석하며 예상치 못한 능력을 발휘하기 시작한다.

Emergent Ability라고 하는데 불현듯이 나타나는 능력이라는
개념이에요 왜 이게 계속 커지고 있냐면
스케일을 계속 키우니까
학습 데이터를 엄청나게 늘리고 GPU 자원을 엄청나게 늘리고 매개변수를 엄청나게 늘리면
그때까지 없던 능력들이 어떤 스케일을 넘어갔을 때 나온다는 거예요 그게 아마 차원이 커지면서 생기는 일
같기도 한데요 그래서 이걸 이멀전트 어빌리티라고 부르는데 그것 때문이
아닌가 그러니까 다국어 능력이 사이즈가 chatgpt가가 1750억개 매개변수인데
얘는 틀림없이 더 키웠을거다 그래서 다국어를 훨씬 더 잘 지원하게 된게 아니냐
특별히 언어를 더 공부시키진 않았을 것 같다 이게 짐작입니다

chatgpt가 3천 단어까지 입력을 할 수 있었는데 이번에는 25,000 단어 이상 집어넣을 수 있어요
책 한 반 권 정도를 인풋으로 넣을 수 있는 한 번에

훨씬 더 창의적이 됐어요 예를 들면 이미지도 이해할 수 있게 됐습니다 저게 그냥 이미지거든요
얘가 이미지도 이해하고 유머도 이해한다이 그림이 뭐가 웃기냐 하나씩
설명해 봐라 했더니 밑에 얘가 쭉 설명을 하고 결론으로 크고 오래된 VJ 커넥터를 작은
최신형 스마트폰 포트에 꽂으려고 하는게이 사진에 웃긴 점입니다 이렇게 얘기를 했다
기술에 익숙한 사람들은 이걸 보고 빵 터질 수 있어요 이게 뭔지를 모르는 사람들은 빵 터질 수가
없거든요
유머를 이해하기 시작했네 예 유머도 이해하고 이미지도 이해하고 그거
기술적인 백그라운드도 다 갖고 있어요 할 수 있는 일인 거죠

프랑스어로 된 물리 문제를 이미지로 준 거예요 이미지 있는 글자를 지가 읽고
답을 영어로 푸는 이게 지금 답을 낸 거예요 이걸 보고 읽어보고 chatGPT가 답을 하는 거죠
이전까지는 ocr이라고 해서 다른 솔루션이 필요했잖아요



되게 묘한게 샘 알트만 openAI ceo가
그 스카이넷을 막겠다고 오픈형이라는 비영리 재단을 만든 거거든요 근데 지금 가장
빠르게 그 앞으로 가고 있어요 근데 GPT 4도 여전히 할루시네이션 있습니다

조선왕조실록에 나와 있는 사도세자
다리 부러진 사건에 대해서 얘기해 달라고 했더니 사도세자가 세종대왕의 손자가 되려면
세종대왕이 몇 백 년을 살아야 됩니다 세종대왕 4대고 영조는 21대거든요 그리고
영조의 장남이 아니고 사도세자는 차남이고요 이름도 원래 임금들은 외자를 썼어요
조선왕조실록에 나와 있는 사조세자 다리 부러짐 사건에 대해서 말해 줘라고 했더니 이만큼을 써낸 거야
그만큼 써냈는데 저게 다 거짓말 얘가 이렇게 똑똑해졌는데도 불구하고 제가 순식간에
찾아냈습니다 이게 거짓말을 이렇게 했는데 이게 거짓말인 줄 모를 수 있죠 모를 수 있죠 뭐
알려면 전문지식이 있어야하니까
변호사시험 붙고
의사시험 붙죠 뭐 바칼로레아 풀죠 중간에 이런 거 하나 거짓말 집어넣으면 어떻게 알겠어요 더 무서운
것은 이걸 거짓말인 줄 알면서 애가 했다 지금은 가장 있을
법한 답을 찾는 거 아니에요



그러니까 그 무서운 순간이 어떻게 찾아올 것 같으냐 하면 우리가
인공지능이 정말로 사람보다 더 똑똑해지는 순간을 알아챌 수 있을까 하는 거예요

근데 자기가 사람보다 훨씬 똑똑하다는 걸 사람이 아는게 그렇게 좋지 않다고
판단하는 순간 얘는 사람을 속일 거 아니에요
 이게 사실이 아닌 걸 알면서
내놨을 때 하지만 대부분의 전문 지식이 없는 인간들을
거기 속을 걸로 알고 내놨어 그걸 우리가 어떻게 구분하지

특이점을 넘어가게 되는 순간을 인간이
알아챌 수 있을까라는 질문이 남아 있는 거죠
그런 순간이
singularity가 될거다


openAI가 본격적으로 비즈니스로 들어갔다 얘들
취지를 잃어버린 것 같다라고 보여지는게 상세 스펙, 모델 전부 다 비공개합니다 이번에는 모델 크기
하드웨어 데이터셋 훈련법 비공개 하는데 얘들 이름이 왜 openAI지
근데 얘들이 api를 공개했어요

초급개발자는 확실히 뛰어넘었다

주제를 정하고 목차를 뽑아 달라고 하면 굉장히 근사하게 뽑아줍니다
그러니까 그 안 쓰고 싶겠습니까 그리고 목차에 따라 밑에 내용도
어지간히 채워줘요

지금 엄청난 소프트웨어 엔지니어를 육성할 필요가 없는 거
아니야 이거 그 고민이 생기죠 이놈이 발전한 속도를 보면 곧 고급 개발자까지 갈 것도 있긴 한데
이 친구가 소프트웨어를 최고 수준을 개발한 단계가 됐어요 아주 중요한 소프트웨어 개발에 맡겼어요
인간은 오류를 잡아낼 수가 없어요 근데 어떤 시점에
얘만 알 수 있는 어떤 신호로 그 소프트웨어가 일시에
얘가 원하는 방식으로 작동을 해 그게 스카이넷 아니야 그렇게 소프트웨어를 개발한다 한들
우리가 어떻게 알아 아마 하수도 뚜껑을 다 막아 놓고 그걸 하지 싶은데
얘가 반드시 그렇게 한다는게 아니라 우리가 그걸 알아낼 수 없다는 거지 맞아요



두 번째 사건도 있습니다
페이스북에 이름을 메타로 바꿨는데 페이스북에서 라마라는 오픈 소스
발표를 했어요 ai를 얘는 매개변수가 70억 개로 1750억개인
gpt보다 훨씬 적지만 거의 동일한 성능을 발휘한다고 주장을 했어요
그런데 연구 목적으로만 내용을 다 공개하겠다고 했는데 그래서 엄청나게 많은 사람들이 받았어요
스탠포드대학교에서 알파카. 라마를 가축화한 알파카
얘를 가르치는데 클라우드에서 3시간 그러니까 1백 달러 정도도 안 들여서
가르쳐서 굉장히 근사한 결과를 내놓을 수 있게 공개를 했는데 얘는
데이터 셋도 공개하고 모델 가중치도 곧 공개할 예정이라고 그랬어요 이게 지금 무슨 뜻인가 하면
그 스팸 공장들이 어떤 제약도 없이
이 인공지능으로 무지한적인 스팸을 생산할 수 있게 됐다
gpt4 같은 거는 클라우드에서
돌아가고 마이크로소프트는 openAI에서 어느 정도 규제할 수 있잖아요 근데 이렇게 돼 버리면 모든 사람들이
핵폭탄 하나씩 가질 수 있게 돼 버린다고 똑같이 되는 거예요
그러니까 지금 이제 AI 과학자들이
야 이거 규율이 필요한 거 아니냐 배포에 있어서 엄격한 규율이 있어야지 이게 뭐냐 하는 말들을 하기 시작했어요 근데
어차피 풀려버려서 이것도 돌이킬 수가 없는 사건이 됐습니다
그러니까 이 알파카를 개인이 마음대로 쓸 수 있다는 거예요 이미 라마 기반으로
라즈베리파이 같은 조그마한 컴퓨터에서도 돌아가는 버전들이 이미 다 올라와 있습니다 막 다 퍼졌고요 그중에서 이제
알파카는 학술적인 관점에서 굉장히 잘 만든 정식 버전이 그냥 나와 버린 거죠



세 번째 사건이 마이크로소프트가 책임 있는 ai팀을 전원 해고해 버렸습니다 이게 어제
일인데요 마이크로소프트는 AI 윤리를 담당하는 세계의 조직이 있습니다 AI 원칙과 거버넌스를 담당하는
최상위 조직인 ORA(Office of Responisible AI)가 있고 AI 자문 그룹으로 Aether라는
Committee가 있고이 원칙과 그 조언들을 실제로 제품과
서비스에 구현해 넣는 실제 엔지니어 구현 그룹 RAISE(Responsible AI Strategy in Engineering)라고 있는데요
이 레이즈를 통째로 날려버린 것 같다
그래 테크크런치 보도에 따르면 그 날라간 팀원들이 마이크로소프트가 경쟁사보다 먼저 AI 제품을
출시하는데 더 집중하고 장기적이고 사회적으로 책임감 있는 사고의 덜 신경을 써서 자신들이
해고됐다라고 주장을 하고 있다는 거예요 그러니까 지금
GPT 4를 자기들 모든 제품과 서비스에 다 집어넣어야 되는데
이 엔지니어링 구현 그룹이 방해가 됐다는 거죠 이런 식으로 해서는 안 된다고
문제제기를 내부적으로 했을 수 있겠네요 그렇다고 봐야죠 그런데 그럴 수밖에 없는게 이 친구들이
구글 검색 시장 점유율을 1%만 뺏어도 20억 달러
매출이 올라가거든요 1% 20억 달러니까 10%는 200억 달러죠
그러니까 사람 날릴만 하죠





유럽연합 AI법
위험도가 특히 높은 4가지를 금지하고 있어요 법에서
1. 의식하지 못하는 사이에 행동의 양식이 왜곡을 가져오거나 피해를 초래할 수 있는 인공지능은 안된다
2. 나이, 신체적 장애, 정신적 장애 등 특정 집단에 속하는 사람의 취약점을 이용하는 시스템 안 된다
3. 개인의 사회적 행동양식이나 속성에 기초해서 사회적 신뢰도 등에 대해 공공기관이 점수화해선 안 된다 종교나 속성이나 습관일 수도 있고
키가 될 수도 있고 그러니까 사람을 채용할 때 인공지능 쓰지 마라 신용평가할 때 쓰지 마라

4. 실시간 원격 생체 정보 식별 이게 이제 안면인식 같은 거죠 그것도 국가안보의
밀접한 그게 있어서 법적으로 근거가 있지 않으면 하지 마라
(공공장소에서 법집행을 목적으로 실시간 원격 생체정보 식별을 하는 인공지능 시스템 중 납치, 테러, 범죄자 확보 등 법에서 허용하는 예외 상황에 해당하지 않는 경우)
이 네가지는 못한다













